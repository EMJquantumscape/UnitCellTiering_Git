{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Batches\n",
    "batches = \"UCB004AB\"     #|UCB003AE-US00-24|UCB003AF-US00-24|UCB003AF-US00-26|UCB003AG-US00-02|UCB003AG-US00-15|UCB003AG-US00-34|UCB003AG-US00-35|\"\n",
    "\n",
    "# Define Unit Cells in ML\n",
    "Layers = 3  # Number of unit cells in each MultiSlayer pouch\n",
    "\n",
    "# Specify any cracked cells\n",
    "CrackedCells = [\"UCB003AL-US00-23\", \"UCB003AL-US00-40\", \"UCB003AN-US00-19\"]\n",
    "\n",
    "#Specify any missing cells\n",
    "MissingCells = [\"UCB003AL-US00-77\", \"UCB003AL-US00-63\", \"UCB003AL-US00-26\"]\n",
    "\n",
    "#Group/Pair only cells that have a cell status of \"Waiting\"\n",
    "GroupWaitingOnly = 1 #needs to be 1 to only pair \"Waiting\" cells \n",
    "\n",
    "#Show only yielded cells\n",
    "ShowOnlyYield = 1 #needs to be 1 to only shows cells that yielded according to datahub. Change this variable to show \"non-yielded\" cells as well \n",
    "\n",
    "#Grouping Criteria\n",
    "ASR_range_limit = 2.0   #threshold for ASR range in group\n",
    "ASR_mean_min_limit= 0.5 # threshold for ASR mean-min in group\n",
    "max_attempts = 10   # how many times the pairing loop will run before giving up, it will reorder the remaining cells by ASR instead of Rank after max_attempts/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules used in this script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from qsdc.client import Client\n",
    "#import functions used in this script\n",
    "import query_tray_samples_V4 as query_tray_samples\n",
    "#assert version(\"qs-data-client\") >= \"0.2.15\", \"qsdc version must be greater than 0.2.15\"\n",
    "\n",
    "#Remove warning messages\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "#establish clients \n",
    "qs_client = Client()\n",
    "conn = qs_client.get_mysql_engine()\n",
    "\n",
    "# !pip install --upgrade qs-data-client  ## RUN THIS IF YOU NEED TO UPDATE qs-data-client\n",
    "#Run in terminal if not already logged in:     gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Cell Data From Datahub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query Data from Datahub\n",
    "\n",
    "#Pull cell metrology data from datahub, both standard/auto metrology and manual review\n",
    "dfctq = qs_client.data_hub.get_dataset(dataset = 'MFG-60L-UC-CTQ') ## standard metro review of unit cells\n",
    "dfmr = qs_client.data_hub.get_dataset(dataset = 'MFG-60L-UC-MR') ## manual review of unit cells\n",
    "dfc = qs_client.data_hub.get_dataset(dataset = 'MFG-60L-UNIT-CELL-TEST-CYCLE') ##electrical test data of unit cells\n",
    "dmlg = qs_client.data_hub.get_dataset(dataset = 'MFG-80L-6L-PRODUCTION') ##multilayer info (ML6_id)\n",
    "#dfc = dfc[dfc['US_id'].str.contains(batches, na=False)]\n",
    "cols = [\n",
    "    \"US_id\",\n",
    "    \"TestCycleStart_datetime_first\",\n",
    "    \"TestCycleStart_datetime\",\n",
    "    \"UCT_Version\",\n",
    "    \"idtest_recipe\",\n",
    "    \"RunIndex\",\n",
    "    \"CycleIndex\",\n",
    "    \"dvdt\",\n",
    "    \"recipe_dvdt_range\",\n",
    "    \"CeilingHoldTime\",\n",
    "    \"CE\",\n",
    "    \"CapacityChargeFraction\",\n",
    "    \"CeilingRestVoltage\",\n",
    "    \"AMSDcCapacity\",\n",
    "    \"CycleFailure\",\n",
    "    \"AnyFailure\",\n",
    "    \"MedDcASR\",\n",
    "    \"DischargeCapacity\"\n",
    "]\n",
    "dfc = dfc[dfc['US_id'].str.contains(batches, na=False)].sort_values([\"RunIndex\",\"CycleIndex\"])[cols]\n",
    "\n",
    "# Keep cells from batches of interest and remove those that failed screening\n",
    "filtered_dfctq = dfctq[dfctq['US_id'].str.contains(batches, na=False)] #keep cells that are from the batches of interest\n",
    "filtered_dfmr = dfmr[dfmr['US_id'].str.contains(batches, na=False)] #keep only cells that are from the batches of interest\n",
    "\n",
    "if ShowOnlyYield == 1:\n",
    "    yielded_dfctq = filtered_dfctq[filtered_dfctq['unit_cell_test_yield'] == 1] #keep cells that yielded\n",
    "    yielded_dfmr = filtered_dfmr[filtered_dfmr['unit_cell_test_yield'] == 1] #keep cells that yielded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cell Status/Tier Spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Automated and Manual Review Data into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overwrite automated results with manual review results\n",
    "# First, merge the DataFrames on 'US_id' to align rows\n",
    "merged_df = yielded_dfctq.merge(yielded_dfmr[['US_id', 'edge_thickness_tier_us_mr', 'A1_anode_tier_top_us_mr', 'A1_anode_tier_bottom_us_mr',\n",
    "                                              'cathode_alignment_custom_model_tier_us_mr', 'median_contour_catholyte_pct_us_mr', 'max_f2f_distance_us','disposition_mr', 'failure_modes_mr']], on='US_id', how='left')\n",
    "# Then, overwrite 'edge_thickness_tier_us' in 'filtered_dfctq' where 'edge_thickness_tier_us_mr' has a value\n",
    "merged_df['edge_thickness_tier_us'] = merged_df['edge_thickness_tier_us_mr'].combine_first(merged_df['edge_thickness_tier_us'])\n",
    "# Then, overwrite 'A1_anode_tier_top_us' in 'filtered_dfctq' where 'A1_anode_tier_top_us_mr' has a value\n",
    "merged_df['A1_anode_tier_top_us'] = merged_df['A1_anode_tier_top_us_mr'].combine_first(merged_df['A1_anode_tier_top_us'])\n",
    "# Then, overwrite 'A1_anode_tier_bottom_us' in 'filtered_dfctq' where 'A1_anode_tier_bottom_us_mr' has a value\n",
    "merged_df['A1_anode_tier_bottom_us'] = merged_df['A1_anode_tier_bottom_us_mr'].combine_first(merged_df['A1_anode_tier_bottom_us'])\n",
    "# Then, overwrite 'A1_anode_tier_bottom_us' in 'filtered_dfctq' where 'A1_anode_tier_bottom_us_mr' has a value\n",
    "merged_df['cathode_alignment_custom_model_tier_us'] = merged_df['cathode_alignment_custom_model_tier_us_mr'].combine_first(merged_df['cathode_alignment_custom_model_tier_us'])\n",
    "#merged_df.loc[merged_df['cathode_alignment_custom_model_tier_us_mr'].notna(), 'cathode_alignment_custom_model_tier_us'] = \\\n",
    "    #merged_df['cathode_alignment_custom_model_tier_us_mr']\n",
    "\n",
    "# Then, overwrite 'A1_anode_tier_bottom_us' in 'filtered_dfctq' where 'A1_anode_tier_bottom_us_mr' has a value\n",
    "merged_df['median_contour_catholyte_pct_us'] = merged_df['median_contour_catholyte_pct_us_mr'].combine_first(merged_df['median_contour_catholyte_pct_us'])\n",
    "# Then, overwrite 'disposition_us' in 'disposition_mr' has a value\n",
    "merged_df['disposition'] = merged_df['disposition_mr'].combine_first(merged_df['disposition'])\n",
    "# Then, overwrite 'disposition_us' in 'disposition_mr' has a value\n",
    "merged_df['failure_modes'] = merged_df['failure_modes_mr'].combine_first(merged_df['failure_modes'])\n",
    "# Drop the 'edge_thickness_tier_us_mr' column if you don't need it\n",
    "dfctq_updated = merged_df.drop(columns=['edge_thickness_tier_us_mr', 'A1_anode_tier_top_us_mr', 'A1_anode_tier_bottom_us_mr',\n",
    "                                              'cathode_alignment_custom_model_tier_us_mr', 'median_contour_catholyte_pct_us_mr', 'disposition_mr','failure_modes_mr' ])\n",
    "\n",
    "\n",
    "#Update Final Tier of Cells\n",
    "conditions = [\n",
    "    dfctq_updated['disposition'] == 'Tier 1',\n",
    "    dfctq_updated['disposition'] == 'Tier 2',\n",
    "    dfctq_updated['disposition'] == 'Fail',\n",
    "    dfctq_updated['disposition'] == 'Scrap',\n",
    "    dfctq_updated['disposition'] == 'Missing Data',\n",
    "]\n",
    "choices = ['1', '2', '3','Scrapped', 'TBD']\n",
    "dfctq_updated['Tier'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Tiering Metrics, Cell Status, Electrical Metrics, and Tray Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill in tiering metrics based on data pulled from datahub\n",
    "\n",
    "## Rename Spreadsheet and sample/batch columns\n",
    "CellTiering = dfctq_updated[['US_id']].rename(columns={'US_id': 'Cell ID'}) \n",
    "CellTiering['Batch'] = dfctq_updated['US_process_flow'] #Create Tiering Spreadsheet\n",
    "# Create columns for final spreadsheet\n",
    "new_columns = [\"Cell Status\", \"Cell Tier\", \"Edge Wetting\", \"Thickness\", \"Alignment\", \"Anode\",\"Film-to-Film Distance\"]\n",
    "for col in new_columns:\n",
    "    CellTiering[col] = np.nan\n",
    "\n",
    "\n",
    "##Update Cell Tiering metrics in final spreadsheet\n",
    "# Merge the dfctq_updated with the CellTiering\n",
    "merged_df = CellTiering.merge(\n",
    "    dfctq_updated[['US_id', 'Tier', 'median_contour_catholyte_pct_us', 'edge_thickness_tier_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us', \n",
    "                   'cathode_alignment_custom_model_tier_us', 'A1_anode_tier_top_us','A1_anode_tier_bottom_us', 'max_f2f_distance_us']],\n",
    "    left_on='Cell ID',\n",
    "    right_on='US_id',\n",
    "    how='left'\n",
    ")\n",
    "# Update final 'Cell Tier' columns with data from dfctq_updated\n",
    "merged_df['Cell Tier'] = merged_df['Tier']\n",
    "#Update 'Edge Wetting' column\n",
    "EWconditions = [\n",
    "    merged_df['median_contour_catholyte_pct_us'] < 80,\n",
    "    (merged_df['median_contour_catholyte_pct_us'] >= 80) & (merged_df['median_contour_catholyte_pct_us'] <= 98),\n",
    "    merged_df['median_contour_catholyte_pct_us'] > 98\n",
    "]\n",
    "EWchoices = [3, 2, 1]\n",
    "merged_df['Edge Wetting'] = np.select(EWconditions, EWchoices)\n",
    "#Update 'Thickness' Column\n",
    "merged_df['Thickness'] = merged_df['edge_thickness_tier_us']\n",
    "#Update Alignment' Column\n",
    "merged_df['Alignment'] = merged_df['cathode_alignment_custom_model_tier_us']\n",
    "#Update 'Anode' Column\n",
    "merged_df['Anode'] = np.maximum(merged_df['A1_anode_tier_top_us'], merged_df['A1_anode_tier_bottom_us'])\n",
    "#Update 'Film-to-Film Distance Column\n",
    "#merged_df['Film-to-Film Distance'] = 'Low'  # Default value\n",
    "merged_df.loc[merged_df['max_f2f_distance_us'] > 0.40, 'Film-to-Film Distance'] = 'High'\n",
    "# Drop the extra columns from dfctq_updated\n",
    "CellTiering = merged_df.drop(columns=['US_id', 'Tier', 'edge_thickness_tier_us','cathode_alignment_custom_model_tier_us',\n",
    "                                      'A1_anode_tier_top_us','A1_anode_tier_bottom_us','max_f2f_distance_us'])\n",
    "\n",
    "## Asign Final Tier to every cell\n",
    "# Select columns that we are considering for tiering\n",
    "columns_to_consider = ['Alignment', 'Anode', 'Thickness', 'Edge Wetting']\n",
    "CellTiering['Alignment'] = CellTiering['Alignment'].fillna(0)  # Replace NaN with 0 (or another placeholder)\n",
    "CellTiering['Alignment'] = CellTiering['Alignment'].astype(int) #convert to integer\n",
    "CellTiering['Anode'] = CellTiering['Anode'].fillna(0)  # Replace NaN with 0 (or another placeholder)\n",
    "CellTiering['Anode'] = CellTiering['Anode'].astype(int) #convert to integer\n",
    "CellTiering['Thickness'] = CellTiering['Thickness'].fillna(0)  # Replace NaN with 0 (or another placeholder)\n",
    "CellTiering['Thickness'] = CellTiering['Thickness'].astype(int) #convert to integer\n",
    "CellTiering['Edge Wetting'] = CellTiering['Edge Wetting'].fillna(0)\n",
    "CellTiering['Edge Wetting'] = CellTiering['Edge Wetting'].astype(int) #convert to integer\n",
    "#Fill out Cell Tier\n",
    "CellTiering['Cell Tier'] = CellTiering[columns_to_consider].max(axis=1) #Update cell tier based on avaialable tiering metrics\n",
    "missing_or_zero = (CellTiering[columns_to_consider].isnull() | (CellTiering[columns_to_consider] == 0)).any(axis=1) #Identify cells that are missing data\n",
    "CellTiering.loc[missing_or_zero, 'Cell Tier'] = 0 # Update 'Cell Status' for these rows\n",
    "\n",
    "\n",
    "##Update 'Cell Status' if yielded cell is on already in a multilayer pouch, has high F2F Misalignment, or is Cracked/Scrapped\n",
    "#Update Cell if it is already in ML\n",
    "merged_df = CellTiering.merge(dmlg, left_on='Cell ID', right_on='US_id', how='left') # Merge 'CellTiering' with 'dmlg' based on matching 'Cell ID' and 'US_id'\n",
    "CellTiering['Cell Status'] = merged_df['ML6_id'].combine_first(CellTiering['Cell Status']) # Update 'Cell Status' in 'CellTiering' with corresponding 'ML6_id' where there is a match\n",
    "#Update Cell if it has high F2F Misalignment\n",
    "CellTiering.loc[CellTiering['Film-to-Film Distance'] == 'High', 'Cell Status'] = 'High F2F Alignment' \n",
    "#Update Cell if it is cracked\n",
    "CellTiering.loc[CellTiering['Cell ID'].isin(CrackedCells), 'Cell Status'] = 'Cracked/Scrapped'\n",
    "#Update Cell if it is missing\n",
    "CellTiering.loc[CellTiering['Cell ID'].isin(MissingCells), 'Cell Status'] = 'Missing/Lost'\n",
    "# Identify rows where any of the specified columns are missing or contain 0\n",
    "columns_to_check = ['Edge Wetting', 'Thickness', 'Alignment', 'Anode']\n",
    "missing_or_zero = (CellTiering[columns_to_check].isnull() | (CellTiering[columns_to_check] == 0)).any(axis=1)\n",
    "CellTiering.loc[missing_or_zero, 'Cell Status'] = 'Missing Tiering Metrics' # Update 'Cell Status' for these rows\n",
    "#Fill in default status of 'Waiting'\n",
    "CellTiering['Cell Status'] = CellTiering['Cell Status'].fillna('Waiting')\n",
    "\n",
    "\n",
    "## Update cell electrical metrics\n",
    "def last_non_missing(series):\n",
    "    return series.dropna().iloc[-1] if not series.dropna().empty else None\n",
    "# Group by 'US_id' and aggregate using the last non-missing values\n",
    "dfc_consolidated = (\n",
    "    dfc.groupby('US_id', as_index=False)\n",
    "    .agg({\n",
    "        'MedDcASR': last_non_missing,\n",
    "        'DischargeCapacity': last_non_missing\n",
    "    })\n",
    ")\n",
    "dfc_consolidated = dfc_consolidated.rename(columns={'US_id': 'Cell ID','MedDcASR': 'MedDischargeASR_1C', 'DischargeCapacity': 'DischargeCapacity_Co3'})# Consolidate column renaming in a single step\n",
    "CellTiering = pd.merge(CellTiering, dfc_consolidated, left_on='Cell ID', right_on='Cell ID', how='left')\n",
    "\n",
    "\n",
    "## Update tray location ##\n",
    "sample_names_group = CellTiering[\"Cell ID\"]\n",
    "df_tray = query_tray_samples.get_sample_tray(sample_names_group)\n",
    "CellTiering = pd.merge(CellTiering, df_tray[['sample_name', 'tray_id', 'row_index', 'col_index', 'modified']], left_on='Cell ID', right_on='sample_name', how='left')\n",
    "CellTiering = CellTiering.drop(columns=['sample_name', 'modified'])\n",
    "\n",
    "#Update if yielded cell is on 2L Reliability\n",
    "#data = data[[\"Cell ID\",\"Reliability Test Count\"]]\n",
    "# Merge the two dataframes on the 'Cell ID' column\n",
    "#merged_df = pd.merge(CellTiering, data, on='Cell ID', how='right')\n",
    "# Update 'Cell Status' where 'Reliability Test Count' is 1 in merged dataframe\n",
    "#merged_df.loc[merged_df['Reliability Test Count'] == 1, 'Cell Status'] = '2L Reliability'\n",
    "# Drop the added columns to keep only the original columns from YieldedCells\n",
    "#YieldedCells = merged_df.drop(columns=['Reliability Test Count'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save spreadsheet showing 'Cell Status' and 'Tier' for all cells in the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save Tiering spreadsheet\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_{batches}_Tier.xlsx'\n",
    "CellTiers = CellTiering.copy()\n",
    "#CellTiers = CellTiers.drop(columns=['median_contour_catholyte_pct_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us']) #drop rows that we don't need to show in the spreadsheet\n",
    "CellTiers = CellTiers.rename(columns={'median_contour_catholyte_pct_us': 'Edge Wetting Pct','center_normalized_0_5mm_eroded_rect_outside_median_us': 'Normalized Thickness', 'Film-to-Film Distance':'F2F Distance'})\n",
    "CellTiers['Cell Tier'] = pd.to_numeric(CellTiers['Cell Tier'], errors='coerce')\n",
    "CellTiers = CellTiers.sort_values(by=['Cell Tier', 'MedDischargeASR_1C'], ascending=[True, True])\n",
    "CellTiers.to_excel(output_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Eligible Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter cells that are eligible for grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format and prepare dataframes for TOPSIS Rank and Grouping\n",
    "\n",
    "#Rename \n",
    "TieredCells = CellTiering\n",
    "\n",
    "#Keep Cells that have not been grouped\n",
    "if GroupWaitingOnly == 1:\n",
    "    TieredCells = TieredCells[TieredCells[\"Cell Status\"] == \"Waiting\"]\n",
    "\n",
    "#Keep Tier 1 Cells to commence pairing of Tier 1 groups\n",
    "TieredOneCells = TieredCells[TieredCells['Cell Tier'] == 1].copy()\n",
    "TieredTwoCells = TieredCells[TieredCells['Cell Tier'] == 2].copy()\n",
    "TieredThreeCells = TieredCells[TieredCells['Cell Tier'] == 3].copy()\n",
    "\n",
    "# Remove duplicate rows across all columns\n",
    "TieredOneCells = TieredOneCells.drop_duplicates()\n",
    "TieredTwoCells = TieredTwoCells.drop_duplicates()\n",
    "TieredThreeCells = TieredThreeCells.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 1: Calculate Cell Score and Ranking using TOPSIS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate Cell Score and Ranking for Tier 1 Cells using TOPSIS method\n",
    "# Define the criteria and their corresponding optimization directions\n",
    "criteria = ['median_contour_catholyte_pct_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us', 'MedDischargeASR_1C', 'DischargeCapacity_Co3'] \n",
    "weights = [0.15, 0.25, 0.35, 0.25]  # Equal weights (adjust if needed)\n",
    "optimization_directions = ['High', 'Proximity', 'Low', 'High']  # 'Proximity' for 'Thickness'\n",
    "target_criteria = ['center_normalized_0_5mm_eroded_rect_outside_median_us']  # Criteria optimized for proximity\n",
    "targets = [1.0]  # Target value for 'Thickness'\n",
    "\n",
    "\n",
    "def topsis_with_target_Tier(df, criteria, weights, optimization_directions, target_criteria=None, targets=None):\n",
    "    \n",
    "    # Ensure numeric data for criteria\n",
    "    df[criteria] = df[criteria].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    normalized_matrix = df[criteria].copy()\n",
    "    \n",
    "    # Step 1: Normalize the Decision Matrix\n",
    "    for col in criteria:\n",
    "        if target_criteria and col in target_criteria:\n",
    "            # Calculate absolute deviation from target\n",
    "            target = targets[target_criteria.index(col)]\n",
    "            normalized_matrix[col] = np.abs(df[col] - target)  # Deviation\n",
    "        else:\n",
    "            # Standard normalization\n",
    "            normalized_matrix[col] = normalized_matrix[col] / np.sqrt((normalized_matrix[col]**2).sum())\n",
    "    \n",
    "    # Step 2: Weight the Criteria\n",
    "    weighted_matrix = normalized_matrix * weights\n",
    "    \n",
    "    # Step 3: Identify Ideal and Negative Ideal Solutions\n",
    "    ideal_solution = []\n",
    "    negative_ideal_solution = []\n",
    "    for col, direction in zip(criteria, optimization_directions):\n",
    "        if target_criteria and col in target_criteria:\n",
    "            # For deviation-based criteria, minimize the deviation\n",
    "            ideal_solution.append(weighted_matrix[col].min())\n",
    "            negative_ideal_solution.append(weighted_matrix[col].max())\n",
    "        else:\n",
    "            if direction == 'High':\n",
    "                ideal_solution.append(weighted_matrix[col].max())\n",
    "                negative_ideal_solution.append(weighted_matrix[col].min())\n",
    "            elif direction == 'Low':\n",
    "                ideal_solution.append(weighted_matrix[col].min())\n",
    "                negative_ideal_solution.append(weighted_matrix[col].max())\n",
    "    \n",
    "    ideal_solution = np.array(ideal_solution)\n",
    "    ideal_solution = np.array([\n",
    "        weighted_matrix.iloc[:, idx].max() if direction == \"maximize\" else weighted_matrix.iloc[:, idx].min()\n",
    "        for idx, direction in enumerate(optimization_directions)\n",
    "    ])\n",
    "\n",
    "    negative_ideal_solution = np.array(negative_ideal_solution)\n",
    "    negative_ideal_solution = np.array([\n",
    "        weighted_matrix.iloc[:, idx].min() if direction == \"maximize\" else weighted_matrix.iloc[:, idx].max()\n",
    "        for idx, direction in enumerate(optimization_directions)\n",
    "    ])\n",
    "    \n",
    "    # Step 4: Calculate Distances\n",
    "    distances_to_ideal = np.sqrt(((weighted_matrix - ideal_solution)**2).sum(axis=1))\n",
    "    distances_to_negative = np.sqrt(((weighted_matrix - negative_ideal_solution)**2).sum(axis=1))\n",
    "    \n",
    "    # Step 5: Calculate TOPSIS Score\n",
    "    topsis_score = distances_to_negative / (distances_to_ideal + distances_to_negative)\n",
    "    return topsis_score\n",
    "\n",
    "# Calculate TOPSIS scores\n",
    "TieredOneCells['total_rank_score'] = topsis_with_target_Tier(TieredOneCells, criteria, weights, optimization_directions, target_criteria, targets)\n",
    "# Rank alternatives by TOPSIS score (higher score is better)\n",
    "TieredOneCells['Rank'] = TieredOneCells['total_rank_score'].rank(ascending=False)\n",
    "# Arrange Cells by best ranking at the top\n",
    "TieredOneCells.sort_values(by='Rank', ascending=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 1: Group Cells using ASR Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TieredOneCells\n",
    "df = df.reset_index(drop=True)\n",
    "df['Rank'] = (df.index + 1).astype(int)  # Add 1-based rank as integers\n",
    "remainder = df.copy()  # Start with a copy of the input DataFrame\n",
    "\n",
    "n = Layers\n",
    "\n",
    "#Initialize variable and dataframe \n",
    "groupnumber = 1  # Initialize group number\n",
    "df_final = pd.DataFrame()  # Initialize an empty DataFrame for the final result\n",
    "grouped_list = [] # List to collect all the groups for final concatenation \n",
    "attempts = 0\n",
    "\n",
    "# Keep track of processed indices\n",
    "processed_indices = set()\n",
    "\n",
    "while max_attempts > attempts:\n",
    "    cellstart = 0\n",
    "    attempts = attempts+1\n",
    "\n",
    "    if attempts > max_attempts/2:\n",
    "        remainder = remainder.sort_values(by='MedDischargeASR_1C', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    while len(remainder) >= n + cellstart:\n",
    "        # Find the next candidate group by slicing remainder\n",
    "        candidate_group = remainder.iloc[cellstart:cellstart+n].copy()\n",
    "        print(f\"Processing group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "\n",
    "        # Calculate criteria for the group\n",
    "        asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "        asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "\n",
    "        print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "\n",
    "        # Check if group satisfies criteria and replace cells if necessary\n",
    "        if asr_range >= ASR_range_limit:\n",
    "            # Replace the highest ASR cell\n",
    "            max_idx = candidate_group['MedDischargeASR_1C'].idxmax()\n",
    "            problem_cell_id = candidate_group.loc[max_idx, 'Cell ID']\n",
    "            print(f\"ASR range is too high at {asr_range}, replacing problem cell: {problem_cell_id}\")\n",
    "            # Replace the highest ASR cell here (you can replace it based on your logic)\n",
    "            i_range = 0\n",
    "\n",
    "            while asr_range >=ASR_range_limit:\n",
    "                if n + i_range >= len(remainder):  # Check if the index is valid\n",
    "                    print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                    cellstart = cellstart +1\n",
    "                    break\n",
    "                candidate_group.loc[max_idx] = remainder.iloc[n + i_range] # Replace the highest ASR cell\n",
    "                replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Cell ID of the replacement cell\n",
    "                print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "                # Check for duplicates in 'Cell ID' within the candidate group\n",
    "                duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                while duplicate_count > 0:\n",
    "                    print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_range.\")\n",
    "                    i_range += 1  # Increment if there are duplicates\n",
    "                    if n + i_range < len(remainder):\n",
    "                        candidate_group.loc[max_idx] = remainder.iloc[n + i_range]\n",
    "                        replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Replacement cell ID\n",
    "                        print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                        duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                    else:\n",
    "                        cellstart = cellstart +1\n",
    "                        break\n",
    "                asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "                i_range = i_range+1\n",
    "\n",
    "        if asr_mean_min >= ASR_mean_min_limit:\n",
    "            # Replace the lowest ASR cell\n",
    "            min_idx = candidate_group['MedDischargeASR_1C'].idxmin()\n",
    "            problem_cell_id = candidate_group.loc[min_idx, 'Cell ID']\n",
    "            print(f\"Minimum ASR is too low at difference of {asr_mean_min}, replacing problem cell: {problem_cell_id}\")\n",
    "            # Replace the lowest ASR cell here (you can replace it based on your logic)\n",
    "            i_mean_min = 0\n",
    "\n",
    "            while asr_mean_min >= ASR_mean_min_limit:\n",
    "                if n + i_mean_min >= len(remainder):  # Check if the index is valid\n",
    "                    print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                    cellstart = cellstart +1\n",
    "                    break\n",
    "                candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "                replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "                print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "                # Check for duplicates in 'Cell ID' within the candidate group\n",
    "                duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                while duplicate_count > 0:\n",
    "                    print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_mean_min.\")\n",
    "                    i_mean_min += 1  # Increment if there are duplicates\n",
    "                    if n + i_mean_min < len(remainder):\n",
    "                        candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "                        replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "                        print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                        duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                    else:\n",
    "                        cellstart = cellstart +1\n",
    "                        break\n",
    "                \n",
    "                asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "                i_mean_min = i_mean_min+1\n",
    "\n",
    "        duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "\n",
    "        if asr_range < ASR_range_limit and asr_mean_min < ASR_mean_min_limit and duplicate_count == 0:\n",
    "            print(f\"Established group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "            # Assign the group number\n",
    "            candidate_group['Group Number'] = groupnumber\n",
    "            # Add the rows to the final list\n",
    "            grouped_list.append(candidate_group)\n",
    "            # Drop rows in \"remainder\" where \"Cell ID\" matches any in \"candidate_group\"\n",
    "            remainder = remainder[~remainder['Cell ID'].isin(candidate_group['Cell ID'])]\n",
    "            # Update processed indices\n",
    "            processed_indices.update(candidate_group.index)\n",
    "            # Update group number for the next group\n",
    "            groupnumber += 1\n",
    "            # Print status\n",
    "            print(f\"Rows in remainder after processing: {len(remainder)}\")\n",
    "\n",
    "# After loop, concatenate all the groups into a single DataFrame\n",
    "# Check if grouped_list contains valid DataFrames\n",
    "if not grouped_list:\n",
    "    print(\"No valid groups were formed. Creating an empty DataFrame.\")\n",
    "    df_final = pd.DataFrame()  # Handle case where no groups are created\n",
    "else:\n",
    "    df_final = pd.concat(grouped_list, ignore_index=True)\n",
    "    # Calculate the average Rank for each Group Number\n",
    "    group_avg_rank = df_final.groupby('Group Number')['Rank'].mean().reset_index()\n",
    "    # Merge the average Rank back into the original DataFrame\n",
    "    df_final = df_final.merge(group_avg_rank, on='Group Number', suffixes=('', '_avg'))\n",
    "    # Sort by the average Rank and then by the original Rank for ties\n",
    "    df_final = df_final.sort_values(by=['Rank_avg', 'Rank'], ascending=True).reset_index(drop=True)\n",
    "    # Assign new Group Numbers based on the sorted order\n",
    "    group_number_mapping = {old: new for new, old in enumerate(df_final['Group Number'].unique(), start=1)}\n",
    "    df_final['Group Number'] = df_final['Group Number'].map(group_number_mapping)\n",
    "    # Drop the temporary average column if not needed\n",
    "    df_final = df_final.drop(columns=['Rank_avg'])\n",
    "    \n",
    "# Add 'Remainder' label to remaining rows, if applicable\n",
    "if not remainder.empty:\n",
    "    remainder['Group Number'] = 'Remainder'\n",
    "    df_final = pd.concat([df_final, remainder], ignore_index=True)\n",
    "\n",
    "\n",
    "if not df_final.empty:\n",
    "    # Reset the index\n",
    "    df_final = df_final.reset_index(drop=True)    \n",
    "    # Drop unnecessary columns\n",
    "    #columns_to_drop = ['median_contour_catholyte_pct_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us', 'total_rank_score']\n",
    "    columns_to_drop = ['total_rank_score', 'Rank']\n",
    "    df_final = df_final.drop(columns=columns_to_drop, errors='ignore')  # Use 'errors=\"ignore\"' to avoid errors if columns are missing\n",
    "    #Rename some columns\n",
    "    df_final = df_final.rename(columns={'median_contour_catholyte_pct_us': 'Edge Wetting Pct','center_normalized_0_5mm_eroded_rect_outside_median_us': 'Normalized Thickness', 'Film-to-Film Distance':'F2F Distance'})\n",
    "    # Move 'Group Number' to the first column\n",
    "    df_final = df_final[['Group Number'] + [col for col in df_final.columns if col != 'Group Number']]\n",
    "\n",
    "\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(f\"Final Tier 1 DataFrame has {df_final.shape[0]} rows.\")\n",
    "\n",
    "# Save the ranked cells to an Excel file\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_{batches}_Tier_1_Grouped.xlsx'\n",
    "#df_final.to_excel(output_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 1: Arrange cells in each group by \"Pyramid\" order of ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_order(group):\n",
    "    # Check if the group number is 'Remainder'\n",
    "    group_number = group['Group Number'].iloc[0]\n",
    "    if group_number == 'Remainder':\n",
    "        # Sort rows in ascending order\n",
    "        sorted_group = group.sort_values(by='MedDischargeASR_1C', ascending=True).reset_index(drop=True)\n",
    "        sorted_group['Group Number'] = sorted_group['Group Number'].astype(str) + \".\" + (sorted_group.index + 1).astype(str)\n",
    "        return sorted_group\n",
    "    \n",
    "    # Sort values within the group\n",
    "    sorted_group = group.sort_values(by='MedDischargeASR_1C', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    # Rearrange in perfect pyramid order\n",
    "    pyramid = [None] * len(sorted_group)  # Preallocate the pyramid\n",
    "    left, right = 0, len(pyramid) - 1     # Pointers for start and end of the pyramid\n",
    "\n",
    "    for i, row in sorted_group.iterrows():\n",
    "        if i % 2 == 0:  # Even index, place at the left\n",
    "            pyramid[left] = row\n",
    "            left += 1\n",
    "        else:           # Odd index, place at the right\n",
    "            pyramid[right] = row\n",
    "            right -= 1\n",
    "    \n",
    "    # Reconstruct DataFrame\n",
    "    pyramid_df = pd.DataFrame(pyramid).reset_index(drop=True)\n",
    "    \n",
    "    # Update Group Number to include position in group\n",
    "    pyramid_df['Group Number'] = group['Group Number'].iloc[0]  # Keep the group number\n",
    "    pyramid_df['Group Number'] = pyramid_df['Group Number'].astype(str) + \".\" + (pyramid_df.index + 1).astype(str)\n",
    "    \n",
    "    return pyramid_df\n",
    "\n",
    "\n",
    "if not df_final.empty:\n",
    "    # Apply the function to each group\n",
    "    df_pyramid = df_final.groupby('Group Number', group_keys=False).apply(pyramid_order)\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_pyramid = df_pyramid.reset_index(drop=True)\n",
    "\n",
    "    # Save the ranked cells to an Excel file\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    output_name = f'{current_date}_{batches}_Tier_1_Grouped_Pyramid_Order.xlsx'\n",
    "    df_pyramid.to_excel(output_name, index=False)\n",
    "else:\n",
    "    print(\"No Tier 1 dataframe to rearrange\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 2: Calculate Cell Score and Ranking using TOPSIS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scoring rules as dictionaries\n",
    "alignment_scores = {1: 0, 2: 0}  # 1->0, 2->0\n",
    "anode_scores = {1: 0, 2: 0.39}   # 1->0, 2->0.39\n",
    "thickness_scores = {1: 0, 2: 0}  # 1->0, 2->0\n",
    "edge_wetting_scores = {1: 0, 2: 0.06}  # 1->0, 2->0.06\n",
    "\n",
    "# Calculate individual scores for each column\n",
    "TieredTwoCells['AlignmentScore'] = TieredTwoCells['Alignment'].map(alignment_scores)\n",
    "TieredTwoCells['AnodeScore'] = TieredTwoCells['Anode'].map(anode_scores)\n",
    "TieredTwoCells['ThicknessScore'] = TieredTwoCells['Thickness'].map(thickness_scores)\n",
    "TieredTwoCells['EdgeWettingScore'] = TieredTwoCells['Edge Wetting'].map(edge_wetting_scores)\n",
    "\n",
    "# Calculate the total score\n",
    "TieredTwoCells['SumOfSpecPoints'] = (\n",
    "    TieredTwoCells['AlignmentScore'] +\n",
    "    TieredTwoCells['AnodeScore'] +\n",
    "    TieredTwoCells['ThicknessScore'] +\n",
    "    TieredTwoCells['EdgeWettingScore']\n",
    ")\n",
    "\n",
    "# Drop intermediate score columns (optional)\n",
    "TieredTwoCells = TieredTwoCells.drop(\n",
    "    ['AlignmentScore', 'AnodeScore', 'ThicknessScore', 'EdgeWettingScore'], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "##Calculate Cell Score and Ranking for Tier 2 Cells using TOPSIS method\n",
    "# Define the criteria and their corresponding optimization directions\n",
    "criteria = ['SumOfSpecPoints', 'MedDischargeASR_1C', 'DischargeCapacity_Co3'] \n",
    "weights = [0.70, 0.20, 0.10]  # Equal weights (adjust if needed)\n",
    "optimization_directions = ['Low', 'Low', 'High']  # 'Proximity' for 'Thickness'\n",
    "# Calculate TOPSIS scores\n",
    "TieredTwoCells['total_rank_score'] = topsis_with_target_Tier(TieredTwoCells, criteria, weights, optimization_directions, target_criteria, targets)\n",
    "\n",
    "# Rank alternatives by TOPSIS score (higher score is better)\n",
    "TieredTwoCells['Rank'] = TieredTwoCells['total_rank_score'].rank(ascending=False)\n",
    "# Arrange Cells by best ranking at the top\n",
    "TieredTwoCells.sort_values(by='Rank', ascending=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 2: Group Cells using ASR Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TieredTwoCells\n",
    "df = df.reset_index(drop=True)\n",
    "df['Rank'] = (df.index + 1).astype(int)  # Add 1-based rank as integers\n",
    "remainder = df.copy()  # Start with a copy of the input DataFrame\n",
    "\n",
    "n = Layers\n",
    "\n",
    "#Initialize variable and dataframe \n",
    "groupnumber = 1  # Initialize group number\n",
    "df_final_Two = pd.DataFrame()  # Initialize an empty DataFrame for the final result\n",
    "grouped_list = [] # List to collect all the groups for final concatenation\n",
    "attempts = 0\n",
    "\n",
    "\n",
    "# Keep track of processed indices\n",
    "processed_indices = set()\n",
    "\n",
    "while max_attempts > attempts:\n",
    "    cellstart = 0\n",
    "    attempts = attempts+1\n",
    "\n",
    "    if attempts > max_attempts/2:\n",
    "        remainder = remainder.sort_values(by='MedDischargeASR_1C', ascending=True).reset_index(drop=True)\n",
    "        \n",
    "    while len(remainder) >= n + cellstart:\n",
    "        # Find the next candidate group by slicing remainder\n",
    "        candidate_group = remainder.iloc[cellstart:cellstart+n].copy()\n",
    "        print(f\"Processing group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "\n",
    "        # Calculate criteria for the group\n",
    "        asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "        asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "\n",
    "        print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "\n",
    "        # Check if group satisfies criteria and replace cells if necessary\n",
    "        if asr_range >= ASR_range_limit:\n",
    "            # Replace the highest ASR cell\n",
    "            max_idx = candidate_group['MedDischargeASR_1C'].idxmax()\n",
    "            problem_cell_id = candidate_group.loc[max_idx, 'Cell ID']\n",
    "            print(f\"ASR range is too high at {asr_range}, replacing problem cell: {problem_cell_id}\")\n",
    "            # Replace the highest ASR cell here (you can replace it based on your logic)\n",
    "            i_range = 0\n",
    "\n",
    "            while asr_range >=ASR_range_limit:\n",
    "                if n + i_range >= len(remainder):  # Check if the index is valid\n",
    "                    print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                    cellstart = cellstart +1\n",
    "                    break\n",
    "                candidate_group.loc[max_idx] = remainder.iloc[n + i_range] # Replace the highest ASR cell\n",
    "                replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Cell ID of the replacement cell\n",
    "                print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "                # Check for duplicates in 'Cell ID' within the candidate group\n",
    "                duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                while duplicate_count > 0:\n",
    "                    print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_range.\")\n",
    "                    i_range += 1  # Increment if there are duplicates\n",
    "                    if n + i_range < len(remainder):\n",
    "                        candidate_group.loc[max_idx] = remainder.iloc[n + i_range]\n",
    "                        replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Replacement cell ID\n",
    "                        print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                        duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                    else:\n",
    "                        cellstart = cellstart +1\n",
    "                        break\n",
    "                asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "                i_range = i_range+1\n",
    "\n",
    "        if asr_mean_min >= ASR_mean_min_limit:\n",
    "            # Replace the lowest ASR cell\n",
    "            min_idx = candidate_group['MedDischargeASR_1C'].idxmin()\n",
    "            problem_cell_id = candidate_group.loc[min_idx, 'Cell ID']\n",
    "            print(f\"Minimum ASR is too low at difference of {asr_mean_min}, replacing problem cell: {problem_cell_id}\")\n",
    "            # Replace the lowest ASR cell here (you can replace it based on your logic)\n",
    "            i_mean_min = 0\n",
    "\n",
    "            while asr_mean_min >= ASR_mean_min_limit:\n",
    "                if n + i_mean_min >= len(remainder):  # Check if the index is valid\n",
    "                    print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                    cellstart = cellstart +1\n",
    "                    break\n",
    "                candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "                replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "                print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "                # Check for duplicates in 'Cell ID' within the candidate group\n",
    "                duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                while duplicate_count > 0:\n",
    "                    print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_mean_min.\")\n",
    "                    i_mean_min += 1  # Increment if there are duplicates\n",
    "                    if n + i_mean_min < len(remainder):\n",
    "                        candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "                        replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "                        print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                        duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                    else:\n",
    "                        cellstart = cellstart+1\n",
    "                        break\n",
    "                \n",
    "                asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "                i_mean_min = i_mean_min+1\n",
    "\n",
    "        duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "\n",
    "        if asr_range < ASR_range_limit and asr_mean_min < ASR_mean_min_limit and duplicate_count == 0:\n",
    "            print(f\"Established group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "            # Assign the group number\n",
    "            candidate_group['Group Number'] = groupnumber\n",
    "            # Add the rows to the final list\n",
    "            grouped_list.append(candidate_group)\n",
    "            # Drop rows in \"remainder\" where \"Cell ID\" matches any in \"candidate_group\"\n",
    "            remainder = remainder[~remainder['Cell ID'].isin(candidate_group['Cell ID'])]\n",
    "            # Update processed indices\n",
    "            processed_indices.update(candidate_group.index)\n",
    "            # Update group number for the next group\n",
    "            groupnumber += 1\n",
    "            # Print status\n",
    "            print(f\"Rows in remainder after processing: {len(remainder)}\")\n",
    "\n",
    "\n",
    "# After loop, concatenate all the groups into a single DataFrame\n",
    "# Check if grouped_list contains valid DataFrames\n",
    "if not grouped_list:\n",
    "    print(\"No valid groups were formed. Creating an empty DataFrame.\")\n",
    "    df_final_Two = pd.DataFrame()  # Handle case where no groups are created\n",
    "else:\n",
    "    df_final_Two = pd.concat(grouped_list, ignore_index=True)\n",
    "    ## Reorder Groups by average rank \n",
    "    group_avg_rank = df_final_Two.groupby('Group Number')['Rank'].mean().reset_index() #Calculate the average Rank for each Group Number\n",
    "    df_final_Two = df_final_Two.merge(group_avg_rank, on='Group Number', suffixes=('', '_avg')) # Merge the average Rank back into the original DataFrame\n",
    "    df_final_Two = df_final_Two.sort_values(by=['Rank_avg', 'Rank'], ascending=True).reset_index(drop=True) # Sort by the average Rank and then by the original Rank for ties\n",
    "    group_number_mapping = {old: new for new, old in enumerate(df_final_Two['Group Number'].unique(), start=1)}# Assign new Group Numbers based on the sorted order\n",
    "    df_final_Two['Group Number'] = df_final_Two['Group Number'].map(group_number_mapping)\n",
    "    df_final_Two = df_final_Two.drop(columns=['Rank_avg']) # Drop the temporary average column if not needed\n",
    "\n",
    "# Add 'Remainder' label to remaining rows, if applicable\n",
    "if not remainder.empty:\n",
    "    remainder['Group Number'] = 'Remainder'\n",
    "    df_final_Two = pd.concat([df_final_Two, remainder], ignore_index=True)\n",
    "\n",
    "#Format df_final_Two dataframe\n",
    "if not df_final_Two.empty:\n",
    "    # Reset the index\n",
    "    df_final_Two = df_final_Two.reset_index(drop=True)    \n",
    "    # Drop unnecessary columns\n",
    "    #columns_to_drop = ['median_contour_catholyte_pct_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us', 'total_rank_score']\n",
    "    columns_to_drop = ['total_rank_score', 'Rank', 'SumOfSpecPoints']\n",
    "    df_final_Two = df_final_Two.drop(columns=columns_to_drop, errors='ignore')  # Use 'errors=\"ignore\"' to avoid errors if columns are missing\n",
    "    #Rename some columns\n",
    "    df_final_Two = df_final_Two.rename(columns={'median_contour_catholyte_pct_us': 'Edge Wetting Pct','center_normalized_0_5mm_eroded_rect_outside_median_us': 'Normalized Thickness', 'Film-to-Film Distance':'F2F Distance'})\n",
    "    # Move 'Group Number' to the first column\n",
    "    df_final_Two = df_final_Two[['Group Number'] + [col for col in df_final_Two.columns if col != 'Group Number']]\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(f\"Final Tier 2 DataFrame has {df_final_Two.shape[0]} rows.\")\n",
    "\n",
    "# Save the ranked cells to an Excel file\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_{batches}_Tier_2_Grouped.xlsx'\n",
    "#df_final_Two.to_excel(output_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 2: Arrange cells in each group by \"Pyramid\" order of ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rearrange rows in pyramid order\n",
    "if not df_final_Two.empty:\n",
    "    # Apply the function to each group\n",
    "    df_pyramid_Two = df_final_Two.groupby('Group Number', group_keys=False).apply(pyramid_order)\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_pyramid_Two = df_pyramid_Two.reset_index(drop=True)\n",
    "\n",
    "    # Save the ranked cells to an Excel file\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    output_name = f'{current_date}_{batches}_Tier_2_Grouped_Pyramid_Order.xlsx'\n",
    "    df_pyramid_Two.to_excel(output_name, index=False)\n",
    "else:\n",
    "    print(\"No Tier 2 dataframe to rearrange\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 3: Calculate Cell Score and Ranking using TOPSIS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scoring rules as dictionaries\n",
    "alignment_scores = {1: 0, 2: 0, 3: 0.26}  # scores for Alignment\n",
    "anode_scores = {1: 0, 2: 0.39, 3: 0.70}   # scores for Anode\n",
    "thickness_scores = {1: 0, 2: 0, 3: 0.33}  # scores for Thickness\n",
    "edge_wetting_scores = {1: 0, 2: 0.06, 3: 0.26}  # scores for EdgeWetting\n",
    "\n",
    "# Calculate individual scores for each column\n",
    "TieredThreeCells['AlignmentScore'] = TieredThreeCells['Alignment'].map(alignment_scores)\n",
    "TieredThreeCells['AnodeScore'] = TieredThreeCells['Anode'].map(anode_scores)\n",
    "TieredThreeCells['ThicknessScore'] = TieredThreeCells['Thickness'].map(thickness_scores)\n",
    "TieredThreeCells['EdgeWettingScore'] = TieredThreeCells['Edge Wetting'].map(edge_wetting_scores)\n",
    "\n",
    "# Calculate the total score\n",
    "TieredThreeCells['SumOfSpecPoints'] = (\n",
    "    TieredThreeCells['AlignmentScore'] +\n",
    "    TieredThreeCells['AnodeScore'] +\n",
    "    TieredThreeCells['ThicknessScore'] +\n",
    "    TieredThreeCells['EdgeWettingScore']\n",
    ")\n",
    "\n",
    "# Drop intermediate score columns (optional)\n",
    "TieredThreeCells = TieredThreeCells.drop(\n",
    "    ['AlignmentScore', 'AnodeScore', 'ThicknessScore', 'EdgeWettingScore'], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "##Calculate Cell Score and Ranking for Tier 2 Cells using TOPSIS method\n",
    "# Define the criteria and their corresponding optimization directions\n",
    "criteria = ['SumOfSpecPoints', 'MedDischargeASR_1C', 'DischargeCapacity_Co3'] \n",
    "weights = [0.70, 0.20, 0.10]  # Equal weights (adjust if needed)\n",
    "optimization_directions = ['Low', 'Low', 'High']  # 'Proximity' for 'Thickness'\n",
    "# Calculate TOPSIS scores\n",
    "TieredThreeCells['total_rank_score'] = topsis_with_target_Tier(TieredThreeCells, criteria, weights, optimization_directions, target_criteria, targets)\n",
    "\n",
    "# Rank alternatives by TOPSIS score (higher score is better)\n",
    "TieredThreeCells['Rank'] = TieredThreeCells['total_rank_score'].rank(ascending=False)\n",
    "# Arrange Cells by best ranking at the top\n",
    "TieredThreeCells.sort_values(by='Rank', ascending=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 3: Group Cells using ASR Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TieredThreeCells\n",
    "df = df.reset_index(drop=True)\n",
    "df['Rank'] = (df.index + 1).astype(int)  # Add 1-based rank as integers\n",
    "remainder = df.copy()  # Start with a copy of the input DataFrame\n",
    "retries = 0  # Track retries for reprocessing the remainder\n",
    "\n",
    "n = Layers\n",
    "\n",
    "#Initialize variable and dataframe \n",
    "groupnumber = 1  # Initialize group number\n",
    "df_final_Three = pd.DataFrame()  # Initialize an empty DataFrame for the final result\n",
    "grouped_list = [] # List to collect all the groups for final concatenation\n",
    "\n",
    "attempts = 0\n",
    "\n",
    "# Keep track of processed indices\n",
    "processed_indices = set()\n",
    "\n",
    "\n",
    "while max_attempts > attempts:\n",
    "    cellstart = 0\n",
    "    attempts = attempts+1\n",
    "\n",
    "    if attempts > max_attempts/2:\n",
    "        remainder = remainder.sort_values(by='MedDischargeASR_1C', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    while len(remainder) >= n + cellstart:\n",
    "        # Find the next candidate group by slicing remainder\n",
    "        candidate_group = remainder.iloc[cellstart:cellstart+n].copy()\n",
    "        print(f\"Processing group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "\n",
    "        # Calculate criteria for the group\n",
    "        asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "        asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "\n",
    "        print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "\n",
    "        # Check if group satisfies criteria and replace cells if necessary\n",
    "        if asr_range >= ASR_range_limit:\n",
    "            # Replace the highest ASR cell\n",
    "            max_idx = candidate_group['MedDischargeASR_1C'].idxmax()\n",
    "            problem_cell_id = candidate_group.loc[max_idx, 'Cell ID']\n",
    "            print(f\"ASR range is too high at {asr_range}, replacing problem cell: {problem_cell_id}\")\n",
    "            # Replace the highest ASR cell here (you can replace it based on your logic)\n",
    "            i_range = 0\n",
    "\n",
    "            while asr_range >=ASR_range_limit:\n",
    "                if n + i_range >= len(remainder):  # Check if the index is valid\n",
    "                    print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                    cellstart = cellstart +1\n",
    "                    break\n",
    "                candidate_group.loc[max_idx] = remainder.iloc[n + i_range] # Replace the highest ASR cell\n",
    "                replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Cell ID of the replacement cell\n",
    "                print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "                # Check for duplicates in 'Cell ID' within the candidate group\n",
    "                duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                while duplicate_count > 0:\n",
    "                    print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_range.\")\n",
    "                    i_range += 1  # Increment if there are duplicates\n",
    "                    if n + i_range < len(remainder):\n",
    "                        candidate_group.loc[max_idx] = remainder.iloc[n + i_range]\n",
    "                        replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Replacement cell ID\n",
    "                        print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                        duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                    else:\n",
    "                        cellstart = cellstart+1\n",
    "                        break\n",
    "                asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "                i_range = i_range+1\n",
    "\n",
    "        if asr_mean_min >= ASR_mean_min_limit:\n",
    "            # Replace the lowest ASR cell\n",
    "            min_idx = candidate_group['MedDischargeASR_1C'].idxmin()\n",
    "            problem_cell_id = candidate_group.loc[min_idx, 'Cell ID']\n",
    "            print(f\"Minimum ASR is too low at difference of {asr_mean_min}, replacing problem cell: {problem_cell_id}\")\n",
    "            # Replace the lowest ASR cell here (you can replace it based on your logic)\n",
    "            i_mean_min = 0\n",
    "\n",
    "            while asr_mean_min >= ASR_mean_min_limit:\n",
    "                if n + i_mean_min >= len(remainder):  # Check if the index is valid\n",
    "                    print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                    cellstart = cellstart +1\n",
    "                    break\n",
    "                candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "                replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "                print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "                # Check for duplicates in 'Cell ID' within the candidate group\n",
    "                duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                while duplicate_count > 0:\n",
    "                    print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_mean_min.\")\n",
    "                    i_mean_min += 1  # Increment if there are duplicates\n",
    "                    if n + i_mean_min < len(remainder):\n",
    "                        candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "                        replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "                        print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                        duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                    else:\n",
    "                        cellstart = cellstart+1\n",
    "                        break\n",
    "                \n",
    "                asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "                print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "                i_mean_min = i_mean_min+1\n",
    "\n",
    "\n",
    "        duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "\n",
    "        if asr_range < ASR_range_limit and asr_mean_min < ASR_mean_min_limit and duplicate_count == 0:\n",
    "            print(f\"Established group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "            # Assign the group number\n",
    "            candidate_group['Group Number'] = groupnumber\n",
    "            # Add the rows to the final list\n",
    "            grouped_list.append(candidate_group)\n",
    "            # Drop rows in \"remainder\" where \"Cell ID\" matches any in \"candidate_group\"\n",
    "            remainder = remainder[~remainder['Cell ID'].isin(candidate_group['Cell ID'])]\n",
    "            # Update processed indices\n",
    "            processed_indices.update(candidate_group.index)\n",
    "            # Update group number for the next group\n",
    "            groupnumber += 1\n",
    "            # Print status\n",
    "            print(f\"Rows in remainder after processing: {len(remainder)}\")\n",
    "\n",
    "\n",
    "# After loop, concatenate all the groups into a single DataFrame\n",
    "# Check if grouped_list contains valid DataFrames\n",
    "if not grouped_list:\n",
    "    print(\"No valid groups were formed. Creating an empty DataFrame.\")\n",
    "    df_final_Three = pd.DataFrame()  # Handle case where no groups are created\n",
    "else:\n",
    "    df_final_Three = pd.concat(grouped_list, ignore_index=True)\n",
    "    ## Reorder Groups by average rank \n",
    "    group_avg_rank = df_final_Three.groupby('Group Number')['Rank'].mean().reset_index() #Calculate the average Rank for each Group Number\n",
    "    df_final_Three = df_final_Three.merge(group_avg_rank, on='Group Number', suffixes=('', '_avg')) # Merge the average Rank back into the original DataFrame\n",
    "    df_final_Three = df_final_Three.sort_values(by=['Rank_avg', 'Rank'], ascending=True).reset_index(drop=True) # Sort by the average Rank and then by the original Rank for ties\n",
    "    group_number_mapping = {old: new for new, old in enumerate(df_final_Three['Group Number'].unique(), start=1)}# Assign new Group Numbers based on the sorted order\n",
    "    df_final_Three['Group Number'] = df_final_Three['Group Number'].map(group_number_mapping)\n",
    "    df_final_Three = df_final_Three.drop(columns=['Rank_avg']) # Drop the temporary average column if not needed\n",
    "\n",
    "\n",
    "# Add 'Remainder' label to remaining rows, if applicable\n",
    "if not remainder.empty:\n",
    "    remainder['Group Number'] = 'Remainder'\n",
    "    df_final_Three = pd.concat([df_final_Three, remainder], ignore_index=True)\n",
    "\n",
    "\n",
    "if not df_final_Three.empty:\n",
    "    # Reset the index\n",
    "    df_final_Three = df_final_Three.reset_index(drop=True)    \n",
    "    # Drop unnecessary columns\n",
    "    #columns_to_drop = ['median_contour_catholyte_pct_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us', 'total_rank_score']\n",
    "    columns_to_drop = ['total_rank_score','Rank', 'SumOfSpecPoints']\n",
    "    df_final_Three = df_final_Three.drop(columns=columns_to_drop, errors='ignore')  # Use 'errors=\"ignore\"' to avoid errors if columns are missing\n",
    "    #Rename some columns\n",
    "    df_final_Three = df_final_Three.rename(columns={'median_contour_catholyte_pct_us': 'Edge Wetting Pct','center_normalized_0_5mm_eroded_rect_outside_median_us': 'Normalized Thickness', 'Film-to-Film Distance':'F2F Distance'})\n",
    "    # Move 'Group Number' to the first column\n",
    "    df_final_Three = df_final_Three[['Group Number'] + [col for col in df_final_Three.columns if col != 'Group Number']]\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(f\"Final Tier 3 DataFrame has {df_final_Three.shape[0]} rows.\")\n",
    "\n",
    "# Save the ranked cells to an Excel file\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_{batches}_Tier_3_Grouped.xlsx'\n",
    "#df_final_Three.to_excel(output_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 3: Arrange cells in each group by \"Pyramid\" order of ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rearrange rows in pyramid order\n",
    "if not df_final_Three.empty:\n",
    "    # Apply the function to each group\n",
    "    df_pyramid_Three = df_final_Three.groupby('Group Number', group_keys=False).apply(pyramid_order)\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_pyramid_Three = df_pyramid_Three.reset_index(drop=True)\n",
    "\n",
    "    # Save the ranked cells to an Excel file\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    output_name = f'{current_date}_{batches}_Tier_3_Grouped_Pyramid_Order.xlsx'\n",
    "    df_pyramid_Three.to_excel(output_name, index=False)\n",
    "else:\n",
    "    print(\"No Tier 3 dataframe to rearrange\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
