{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Batches\n",
    "batches = \"UCB003A[K-N]\"#|UCB003AE-US00-24|UCB003AF-US00-24|UCB003AF-US00-26|UCB003AG-US00-02|UCB003AG-US00-15|UCB003AG-US00-34|UCB003AG-US00-35|\"\n",
    "#batches = \"UCB004AA\"\n",
    "\n",
    "# Define Layers\n",
    "Layers = 3  # Number of samples in each group\n",
    "\n",
    "# Specify any cracked cells\n",
    "CrackedCells = [\n",
    "    \"UCB003AL-US00-23\", \"UCB003AL-US00-40\", \"UCB003AN-US00-19\"\n",
    "]\n",
    "\n",
    "MissingCells = [\"UCB003AL-US00-77\", \"UCB003AL-US00-63\", \"UCB003AL-US00-26\"]\n",
    "\n",
    "#Show only yielded cells\n",
    "ShowOnlyYield = \"yes\" #\"yes\" only shows cells that yielded according to datahub. Change this variable to show \"non-yielded\" cells "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules used in this script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import genealogy_v2\n",
    "import query_tray_samples_V4 as query_tray_samples\n",
    "import unit_cell_electrical_yield_and_metrics_v2 as uceym\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from qsdc.client import Client\n",
    "#assert version(\"qs-data-client\") >= \"0.2.15\", \"qsdc version must be greater than 0.2.15\"\n",
    "\n",
    "#Remove warning messages\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "#establish clients \n",
    "qs_client = Client()\n",
    "conn = qs_client.get_mysql_engine()\n",
    "\n",
    "# !pip install --upgrade qs-data-client  ## RUN THIS IF YOU NEED TO UPDATE qs-data-client\n",
    "#Run in terminal if not already logged in:     gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Cell Data From Datahub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two datasets that we care about are:\n",
    "* `MFG-60L-UC-CTQ`. This should Critical to Quality metrics per the <u>[specs](https://qs-cloud.atlassian.net/wiki/spaces/HAB/pages/1782611969/Specifications+A1+Cell+Tiering+-+Post+UCT)</u> provided by Jordi. It contains columns `disposition` and `failure_modes` which should be pretty self-explanatory.\n",
    "* `MFG-60L-UC-MR`. This table should contain the same number of rows (one per cell) as the CTQ table, but the `disposition_mr` and `failure_modes_mr` columns will be replaced with values corresponding to the manual review data. If no manual review was done, the standard metro `diposition` and `failure_modes` will be reflected.This makes it much easier to do 1:1 comparisons across the datasets. We can adjust to include only the manually reviewed cells if that's the preference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EDM02\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\qsdc\\datahub\\datahub.py:88: DtypeWarning: Columns (30,31,32,33,35,36,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(bytesIO(blob_bytes))\n"
     ]
    }
   ],
   "source": [
    "## Query Data from Datahub\n",
    "\n",
    "#Pull cell metrology data from datahub, both standard/auto metrology and manual review\n",
    "dfctq = qs_client.data_hub.get_dataset(dataset = 'MFG-60L-UC-CTQ') ## standard metro review of unit cells\n",
    "dfmr = qs_client.data_hub.get_dataset(dataset = 'MFG-60L-UC-MR') ## manual review of unit cells\n",
    "dfc = qs_client.data_hub.get_dataset(dataset = 'MFG-60L-UNIT-CELL-TEST-CYCLE') ##electrical test data of unit cells\n",
    "dmlg = qs_client.data_hub.get_dataset(dataset = 'MFG-80L-6L-PRODUCTION') ##multilayer info (ML6_id)\n",
    "#dfc = dfc[dfc['US_id'].str.contains(batches, na=False)]\n",
    "cols = [\n",
    "    \"US_id\",\n",
    "    \"TestCycleStart_datetime_first\",\n",
    "    \"TestCycleStart_datetime\",\n",
    "    \"UCT_Version\",\n",
    "    \"idtest_recipe\",\n",
    "    \"RunIndex\",\n",
    "    \"CycleIndex\",\n",
    "    \"dvdt\",\n",
    "    \"recipe_dvdt_range\",\n",
    "    \"CeilingHoldTime\",\n",
    "    \"CE\",\n",
    "    \"CapacityChargeFraction\",\n",
    "    \"CeilingRestVoltage\",\n",
    "    \"AMSDcCapacity\",\n",
    "    \"CycleFailure\",\n",
    "    \"AnyFailure\",\n",
    "    \"MedDcASR\",\n",
    "    \"DischargeCapacity\"\n",
    "]\n",
    "dfc = dfc[dfc['US_id'].str.contains(batches, na=False)].sort_values([\"RunIndex\",\"CycleIndex\"])[cols]\n",
    "\n",
    "# Keep cells from batches of interest and remove those that failed screening\n",
    "filtered_dfctq = dfctq[dfctq['US_id'].str.contains(batches, na=False)] #keep cells that are from the batches of interest\n",
    "filtered_dfmr = dfmr[dfmr['US_id'].str.contains(batches, na=False)] #keep only cells that are from the batches of interest\n",
    "\n",
    "if ShowOnlyYield == \"yes\":\n",
    "    yielded_dfctq = filtered_dfctq[filtered_dfctq['unit_cell_test_yield'] == 1] #keep cells that yielded\n",
    "    yielded_dfmr = filtered_dfmr[filtered_dfmr['unit_cell_test_yield'] == 1] #keep cells that yielded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cell Status/Tier Spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Automated and Manual Review Data into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overwrite automated results with manual review results\n",
    "# First, merge the DataFrames on 'US_id' to align rows\n",
    "merged_df = yielded_dfctq.merge(yielded_dfmr[['US_id', 'edge_thickness_tier_us_mr', 'A1_anode_tier_top_us_mr', 'A1_anode_tier_bottom_us_mr',\n",
    "                                              'cathode_alignment_custom_model_tier_us_mr', 'median_contour_catholyte_pct_us_mr', 'max_f2f_distance_us','disposition_mr', 'failure_modes_mr']], on='US_id', how='left')\n",
    "# Then, overwrite 'edge_thickness_tier_us' in 'filtered_dfctq' where 'edge_thickness_tier_us_mr' has a value\n",
    "merged_df['edge_thickness_tier_us'] = merged_df['edge_thickness_tier_us_mr'].combine_first(merged_df['edge_thickness_tier_us'])\n",
    "# Then, overwrite 'A1_anode_tier_top_us' in 'filtered_dfctq' where 'A1_anode_tier_top_us_mr' has a value\n",
    "merged_df['A1_anode_tier_top_us'] = merged_df['A1_anode_tier_top_us_mr'].combine_first(merged_df['A1_anode_tier_top_us'])\n",
    "# Then, overwrite 'A1_anode_tier_bottom_us' in 'filtered_dfctq' where 'A1_anode_tier_bottom_us_mr' has a value\n",
    "merged_df['A1_anode_tier_bottom_us'] = merged_df['A1_anode_tier_bottom_us_mr'].combine_first(merged_df['A1_anode_tier_bottom_us'])\n",
    "# Then, overwrite 'A1_anode_tier_bottom_us' in 'filtered_dfctq' where 'A1_anode_tier_bottom_us_mr' has a value\n",
    "merged_df['cathode_alignment_custom_model_tier_us'] = merged_df['cathode_alignment_custom_model_tier_us_mr'].combine_first(merged_df['cathode_alignment_custom_model_tier_us'])\n",
    "#merged_df.loc[merged_df['cathode_alignment_custom_model_tier_us_mr'].notna(), 'cathode_alignment_custom_model_tier_us'] = \\\n",
    "    #merged_df['cathode_alignment_custom_model_tier_us_mr']\n",
    "\n",
    "# Then, overwrite 'A1_anode_tier_bottom_us' in 'filtered_dfctq' where 'A1_anode_tier_bottom_us_mr' has a value\n",
    "merged_df['median_contour_catholyte_pct_us'] = merged_df['median_contour_catholyte_pct_us_mr'].combine_first(merged_df['median_contour_catholyte_pct_us'])\n",
    "# Then, overwrite 'disposition_us' in 'disposition_mr' has a value\n",
    "merged_df['disposition'] = merged_df['disposition_mr'].combine_first(merged_df['disposition'])\n",
    "# Then, overwrite 'disposition_us' in 'disposition_mr' has a value\n",
    "merged_df['failure_modes'] = merged_df['failure_modes_mr'].combine_first(merged_df['failure_modes'])\n",
    "# Drop the 'edge_thickness_tier_us_mr' column if you don't need it\n",
    "dfctq_updated = merged_df.drop(columns=['edge_thickness_tier_us_mr', 'A1_anode_tier_top_us_mr', 'A1_anode_tier_bottom_us_mr',\n",
    "                                              'cathode_alignment_custom_model_tier_us_mr', 'median_contour_catholyte_pct_us_mr', 'disposition_mr','failure_modes_mr' ])\n",
    "\n",
    "\n",
    "#Update Final Tier of Cells\n",
    "conditions = [\n",
    "    dfctq_updated['disposition'] == 'Tier 1',\n",
    "    dfctq_updated['disposition'] == 'Tier 2',\n",
    "    dfctq_updated['disposition'] == 'Fail',\n",
    "    dfctq_updated['disposition'] == 'Scrap',\n",
    "    dfctq_updated['disposition'] == 'Missing Data',\n",
    "]\n",
    "choices = ['1', '2', '3','Scrapped', 'TBD']\n",
    "dfctq_updated['Tier'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Tiering Metrics, Cell Status, Electrical Metrics, and Tray Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill in tiering metrics based on data pulled from datahub\n",
    "\n",
    "## Rename Spreadsheet and sample/batch columns\n",
    "CellTiering = dfctq_updated[['US_id']].rename(columns={'US_id': 'Cell ID'}) \n",
    "CellTiering['Batch'] = dfctq_updated['US_process_flow'] #Create Tiering Spreadsheet\n",
    "# Create columns for final spreadsheet\n",
    "new_columns = [\"Cell Status\", \"Cell Tier\", \"Edge Wetting\", \"Thickness\", \"Alignment\", \"Anode\",\"Film-to-Film Distance\"]\n",
    "for col in new_columns:\n",
    "    CellTiering[col] = np.nan\n",
    "\n",
    "\n",
    "##Update Cell Tiering metrics in final spreadsheet\n",
    "# Merge the dfctq_updated with the CellTiering\n",
    "merged_df = CellTiering.merge(\n",
    "    dfctq_updated[['US_id', 'Tier', 'median_contour_catholyte_pct_us', 'edge_thickness_tier_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us', \n",
    "                   'cathode_alignment_custom_model_tier_us', 'A1_anode_tier_top_us','A1_anode_tier_bottom_us', 'max_f2f_distance_us']],\n",
    "    left_on='Cell ID',\n",
    "    right_on='US_id',\n",
    "    how='left'\n",
    ")\n",
    "# Update final 'Cell Tier' columns with data from dfctq_updated\n",
    "merged_df['Cell Tier'] = merged_df['Tier']\n",
    "#Update 'Edge Wetting' column\n",
    "EWconditions = [\n",
    "    merged_df['median_contour_catholyte_pct_us'] < 80,\n",
    "    (merged_df['median_contour_catholyte_pct_us'] >= 80) & (merged_df['median_contour_catholyte_pct_us'] <= 98),\n",
    "    merged_df['median_contour_catholyte_pct_us'] > 98\n",
    "]\n",
    "EWchoices = [3, 2, 1]\n",
    "merged_df['Edge Wetting'] = np.select(EWconditions, EWchoices)\n",
    "#Update 'Thickness' Column\n",
    "merged_df['Thickness'] = merged_df['edge_thickness_tier_us']\n",
    "#Update Alignment' Column\n",
    "merged_df['Alignment'] = merged_df['cathode_alignment_custom_model_tier_us']\n",
    "#Update 'Anode' Column\n",
    "merged_df['Anode'] = np.maximum(merged_df['A1_anode_tier_top_us'], merged_df['A1_anode_tier_bottom_us'])\n",
    "#Update 'Film-to-Film Distance Column\n",
    "#merged_df['Film-to-Film Distance'] = 'Low'  # Default value\n",
    "merged_df.loc[merged_df['max_f2f_distance_us'] > 0.40, 'Film-to-Film Distance'] = 'High'\n",
    "# Drop the extra columns from dfctq_updated\n",
    "CellTiering = merged_df.drop(columns=['US_id', 'Tier', 'edge_thickness_tier_us','cathode_alignment_custom_model_tier_us',\n",
    "                                      'A1_anode_tier_top_us','A1_anode_tier_bottom_us','max_f2f_distance_us'])\n",
    "\n",
    "## Asign Final Tier to every cell\n",
    "# Select columns that we are considering for tiering\n",
    "columns_to_consider = [\n",
    "    'Alignment',\n",
    "    'Anode',\n",
    "    'Thickness', \n",
    "    'Edge Wetting'\n",
    "]\n",
    "CellTiering['Alignment'] = CellTiering['Alignment'].fillna(0)  # Replace NaN with 0 (or another placeholder)\n",
    "CellTiering['Alignment'] = CellTiering['Alignment'].astype(int) #convert to integer\n",
    "CellTiering['Anode'] = CellTiering['Anode'].astype(int) #convert to integer\n",
    "CellTiering['Thickness'] = CellTiering['Thickness'].astype(int) #convert to integer\n",
    "CellTiering['Edge Wetting'] = CellTiering['Edge Wetting'].astype(int) #convert to integer\n",
    "#Fill out Cell Tier\n",
    "CellTiering['Cell Tier'] = CellTiering[columns_to_consider].max(axis=1) #Update cell tier based on avaialable tiering metrics\n",
    "missing_or_zero = (CellTiering[columns_to_consider].isnull() | (CellTiering[columns_to_consider] == 0)).any(axis=1) #Identify cells that are missing data\n",
    "CellTiering.loc[missing_or_zero, 'Cell Tier'] = 0 # Update 'Cell Status' for these rows\n",
    "\n",
    "\n",
    "##Update 'Cell Status' if yielded cell is on already in a multilayer pouch, has high F2F Misalignment, or is Cracked/Scrapped\n",
    "#Update Cell if it is already in ML\n",
    "gen = genealogy_v2.get_genealogy_2L('APD|ML|UC|QSC', conn) # Query dataframe from database\n",
    "us_to_ps_mapping = dict(zip(gen['2L_cell_id'], gen['6L_cell_id'])) # Map the 'US_id' to '6L_cell_id' using '2L_cell_id' in the 'gen' dataframe\n",
    "CellTiering['Cell Status'] = CellTiering['Cell ID'].map(us_to_ps_mapping)\n",
    "#Update Cell if it has high F2F Misalignment\n",
    "CellTiering.loc[CellTiering['Film-to-Film Distance'] == 'High', 'Cell Status'] = 'High F2F Alignment' \n",
    "#Update Cell if it is cracked\n",
    "CellTiering.loc[CellTiering['Cell ID'].isin(CrackedCells), 'Cell Status'] = 'Cracked/Scrapped'\n",
    "#Update Cell if it is missing\n",
    "CellTiering.loc[CellTiering['Cell ID'].isin(MissingCells), 'Cell Status'] = 'Missing/Lost'\n",
    "# Identify rows where any of the specified columns are missing or contain 0\n",
    "columns_to_check = ['Edge Wetting', 'Thickness', 'Alignment', 'Anode']\n",
    "missing_or_zero = (CellTiering[columns_to_check].isnull() | (CellTiering[columns_to_check] == 0)).any(axis=1)\n",
    "CellTiering.loc[missing_or_zero, 'Cell Status'] = 'Missing Tiering Metrics' # Update 'Cell Status' for these rows\n",
    "#Fill in default status of 'Waiting'\n",
    "CellTiering['Cell Status'] = CellTiering['Cell Status'].fillna('Waiting')\n",
    "\n",
    "## Update cell electrical metrics, currently using python scripts to query but will need to transition to datahub using dataframe dfc\n",
    "df_electrical_yield_metrics = uceym.get_electrical_yield_and_metrics(CellTiering[\"Cell ID\"])\n",
    "df_electrical_yield_metrics = df_electrical_yield_metrics[['US_id','MedDischargeASR_1C', 'DischargeCapacity_Co3', 'dVdt_delta_fastcharge']]\n",
    "df_electrical_yield_metrics = df_electrical_yield_metrics.rename(columns={'US_id': 'Cell ID'}) #Create Tiering Spreadsheet\n",
    "CellTiering = pd.merge(CellTiering, df_electrical_yield_metrics, left_on='Cell ID', right_on='Cell ID', how='left')\n",
    "\n",
    "\n",
    "## Update tray location ##\n",
    "sample_names_group = CellTiering[\"Cell ID\"]\n",
    "df_tray = query_tray_samples.get_sample_tray(sample_names_group)\n",
    "CellTiering = pd.merge(CellTiering, df_tray[['sample_name', 'tray_id', 'row_index', 'col_index', 'modified']], left_on='Cell ID', right_on='sample_name', how='left')\n",
    "CellTiering = CellTiering.drop(columns=['sample_name', 'modified'])\n",
    "\n",
    "#Update if yielded cell is on 2L Reliability\n",
    "#data = data[[\"Cell ID\",\"Reliability Test Count\"]]\n",
    "# Merge the two dataframes on the 'Cell ID' column\n",
    "#merged_df = pd.merge(CellTiering, data, on='Cell ID', how='right')\n",
    "# Update 'Cell Status' where 'Reliability Test Count' is 1 in merged dataframe\n",
    "#merged_df.loc[merged_df['Reliability Test Count'] == 1, 'Cell Status'] = '2L Reliability'\n",
    "# Drop the added columns to keep only the original columns from YieldedCells\n",
    "#YieldedCells = merged_df.drop(columns=['Reliability Test Count'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save spreadsheet showing 'Cell Status' and 'Tier' for all cells in the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save Tiering spreadsheet\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_{batches}_Tier.xlsx'\n",
    "CellTiers = CellTiering.copy()\n",
    "CellTiers = CellTiers.drop(columns=['median_contour_catholyte_pct_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us']) #drop rows that we don't need to show in the spreadsheet\n",
    "CellTiers['Cell Tier'] = pd.to_numeric(CellTiers['Cell Tier'], errors='coerce')\n",
    "CellTiers = CellTiers.sort_values(by=['Cell Tier', 'MedDischargeASR_1C'], ascending=[True, True])\n",
    "CellTiers.to_excel(output_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Eligible Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter cells that are eligible for grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format and prepare dataframes for TOPSIS Rank and Grouping\n",
    "\n",
    "#Rename \n",
    "TieredCells = CellTiering\n",
    "\n",
    "#Keep Cells that have not been grouped\n",
    "TieredCells = TieredCells[TieredCells[\"Cell Status\"] == \"Waiting\"]\n",
    "\n",
    "#Keep Tier 1 Cells to commence pairing of Tier 1 groups\n",
    "TieredOneCells = TieredCells[TieredCells['Cell Tier'] == 1].copy()\n",
    "TieredTwoCells = TieredCells[TieredCells['Cell Tier'] == 2].copy()\n",
    "TieredThreeCells = TieredCells[TieredCells['Cell Tier'] == 3].copy()\n",
    "\n",
    "# Remove duplicate rows across all columns\n",
    "TieredOneCells = TieredOneCells.drop_duplicates()\n",
    "TieredTwoCells = TieredTwoCells.drop_duplicates()\n",
    "TieredThreeCells = TieredThreeCells.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 1: Calculate Cell Score and Ranking using TOPSIS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate Cell Score and Ranking for Tier 1 Cells using TOPSIS method\n",
    "# Define the criteria and their corresponding optimization directions\n",
    "criteria = ['median_contour_catholyte_pct_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us', 'MedDischargeASR_1C', 'DischargeCapacity_Co3'] \n",
    "weights = [0.15, 0.25, 0.35, 0.25]  # Equal weights (adjust if needed)\n",
    "optimization_directions = ['High', 'Proximity', 'Low', 'High']  # 'Proximity' for 'Thickness'\n",
    "target_criteria = ['center_normalized_0_5mm_eroded_rect_outside_median_us']  # Criteria optimized for proximity\n",
    "targets = [1.0]  # Target value for 'Thickness'\n",
    "\n",
    "\n",
    "def topsis_with_target_Tier(df, criteria, weights, optimization_directions, target_criteria=None, targets=None):\n",
    "    \n",
    "    # Ensure numeric data for criteria\n",
    "    df[criteria] = df[criteria].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    normalized_matrix = df[criteria].copy()\n",
    "    \n",
    "    # Step 1: Normalize the Decision Matrix\n",
    "    for col in criteria:\n",
    "        if target_criteria and col in target_criteria:\n",
    "            # Calculate absolute deviation from target\n",
    "            target = targets[target_criteria.index(col)]\n",
    "            normalized_matrix[col] = np.abs(df[col] - target)  # Deviation\n",
    "        else:\n",
    "            # Standard normalization\n",
    "            normalized_matrix[col] = normalized_matrix[col] / np.sqrt((normalized_matrix[col]**2).sum())\n",
    "    \n",
    "    # Step 2: Weight the Criteria\n",
    "    weighted_matrix = normalized_matrix * weights\n",
    "    \n",
    "    # Step 3: Identify Ideal and Negative Ideal Solutions\n",
    "    ideal_solution = []\n",
    "    negative_ideal_solution = []\n",
    "    for col, direction in zip(criteria, optimization_directions):\n",
    "        if target_criteria and col in target_criteria:\n",
    "            # For deviation-based criteria, minimize the deviation\n",
    "            ideal_solution.append(weighted_matrix[col].min())\n",
    "            negative_ideal_solution.append(weighted_matrix[col].max())\n",
    "        else:\n",
    "            if direction == 'High':\n",
    "                ideal_solution.append(weighted_matrix[col].max())\n",
    "                negative_ideal_solution.append(weighted_matrix[col].min())\n",
    "            elif direction == 'Low':\n",
    "                ideal_solution.append(weighted_matrix[col].min())\n",
    "                negative_ideal_solution.append(weighted_matrix[col].max())\n",
    "    \n",
    "    ideal_solution = np.array(ideal_solution)\n",
    "    ideal_solution = np.array([\n",
    "        weighted_matrix.iloc[:, idx].max() if direction == \"maximize\" else weighted_matrix.iloc[:, idx].min()\n",
    "        for idx, direction in enumerate(optimization_directions)\n",
    "    ])\n",
    "\n",
    "    negative_ideal_solution = np.array(negative_ideal_solution)\n",
    "    negative_ideal_solution = np.array([\n",
    "        weighted_matrix.iloc[:, idx].min() if direction == \"maximize\" else weighted_matrix.iloc[:, idx].max()\n",
    "        for idx, direction in enumerate(optimization_directions)\n",
    "    ])\n",
    "    \n",
    "    # Step 4: Calculate Distances\n",
    "    distances_to_ideal = np.sqrt(((weighted_matrix - ideal_solution)**2).sum(axis=1))\n",
    "    distances_to_negative = np.sqrt(((weighted_matrix - negative_ideal_solution)**2).sum(axis=1))\n",
    "    \n",
    "    # Step 5: Calculate TOPSIS Score\n",
    "    topsis_score = distances_to_negative / (distances_to_ideal + distances_to_negative)\n",
    "    return topsis_score\n",
    "\n",
    "# Calculate TOPSIS scores\n",
    "TieredOneCells['total_rank_score'] = topsis_with_target_Tier(TieredOneCells, criteria, weights, optimization_directions, target_criteria, targets)\n",
    "# Rank alternatives by TOPSIS score (higher score is better)\n",
    "TieredOneCells['Rank'] = TieredOneCells['total_rank_score'].rank(ascending=False)\n",
    "# Arrange Cells by best ranking at the top\n",
    "TieredOneCells.sort_values(by='Rank', ascending=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 1: Group Cells using ASR Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group starting at rank [1, 2, 3]: ['UCB003AN-US01-11', 'UCB003AM-US00-35', 'UCB003AN-US01-31']\n",
      "ASR range: 0.19709999999999894, ASR mean-min: 0.0813666666666677\n",
      "Established group starting at rank [1, 2, 3]: ['UCB003AN-US01-11', 'UCB003AM-US00-35', 'UCB003AN-US01-31']\n",
      "Rows in remainder after processing: 3\n",
      "Processing group starting at rank [4, 5, 6]: ['UCB003AN-US01-14', 'UCB003AL-US00-70', 'UCB003AM-US00-07']\n",
      "ASR range: 3.3887, ASR mean-min: 1.9090333333333334\n",
      "ASR range is too high at 3.3887, replacing problem cell: UCB003AM-US00-07\n",
      "No more rows in remainder to replace UCB003AM-US00-07.\n",
      "Minimum ASR is too low at difference of 1.9090333333333334, replacing problem cell: UCB003AN-US01-14\n",
      "No more rows in remainder to replace UCB003AN-US01-14.\n",
      "Final Tier 1 DataFrame has 6 rows.\n"
     ]
    }
   ],
   "source": [
    "df = TieredOneCells\n",
    "df = df.reset_index(drop=True)\n",
    "df['Rank'] = (df.index + 1).astype(int)  # Add 1-based rank as integers\n",
    "remainder = df.copy()  # Start with a copy of the input DataFrame\n",
    "retries = 0  # Track retries for reprocessing the remainder\n",
    "\n",
    "n = Layers\n",
    "\n",
    "#Initialize variable and dataframe \n",
    "groupnumber = 1  # Initialize group number\n",
    "df_final = pd.DataFrame()  # Initialize an empty DataFrame for the final result\n",
    "grouped_list = [] # List to collect all the groups for final concatenation\n",
    "cellstart = 0\n",
    "\n",
    "# Keep track of processed indices\n",
    "processed_indices = set()\n",
    "\n",
    "while len(remainder) >= n + cellstart:\n",
    "    # Find the next candidate group by slicing remainder\n",
    "    candidate_group = remainder.iloc[cellstart:cellstart+n].copy()\n",
    "    print(f\"Processing group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "\n",
    "    # Calculate criteria for the group\n",
    "    asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "    asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "\n",
    "    print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "\n",
    "    # Check if group satisfies criteria and replace cells if necessary\n",
    "    if asr_range >= 2.0:\n",
    "        # Replace the highest ASR cell\n",
    "        max_idx = candidate_group['MedDischargeASR_1C'].idxmax()\n",
    "        problem_cell_id = candidate_group.loc[max_idx, 'Cell ID']\n",
    "        print(f\"ASR range is too high at {asr_range}, replacing problem cell: {problem_cell_id}\")\n",
    "        # Replace the highest ASR cell here (you can replace it based on your logic)\n",
    "        i_range = 0\n",
    "\n",
    "        while asr_range >=2.0:\n",
    "            if n + i_range >= len(remainder):  # Check if the index is valid\n",
    "                print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                cellstart = cellstart +1\n",
    "                break\n",
    "            candidate_group.loc[max_idx] = remainder.iloc[n + i_range] # Replace the highest ASR cell\n",
    "            replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Cell ID of the replacement cell\n",
    "            print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "            # Check for duplicates in 'Cell ID' within the candidate group\n",
    "            duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "            while duplicate_count > 0:\n",
    "                print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_range.\")\n",
    "                i_range += 1  # Increment if there are duplicates\n",
    "                if n + i_range < len(remainder):\n",
    "                    candidate_group.loc[max_idx] = remainder.iloc[n + i_range]\n",
    "                    replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Replacement cell ID\n",
    "                    print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                    duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                else:\n",
    "                    break\n",
    "            asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "            i_range = i_range+1\n",
    "\n",
    "    if asr_mean_min >= 0.5:\n",
    "        # Replace the lowest ASR cell\n",
    "        min_idx = candidate_group['MedDischargeASR_1C'].idxmin()\n",
    "        problem_cell_id = candidate_group.loc[min_idx, 'Cell ID']\n",
    "        print(f\"Minimum ASR is too low at difference of {asr_mean_min}, replacing problem cell: {problem_cell_id}\")\n",
    "        # Replace the lowest ASR cell here (you can replace it based on your logic)\n",
    "        i_mean_min = 0\n",
    "\n",
    "        while asr_mean_min >= 0.5:\n",
    "            if n + i_mean_min >= len(remainder):  # Check if the index is valid\n",
    "                print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                cellstart = cellstart +1\n",
    "                break\n",
    "            candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "            replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "            print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "            # Check for duplicates in 'Cell ID' within the candidate group\n",
    "            duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "            while duplicate_count > 0:\n",
    "                print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_mean_min.\")\n",
    "                i_mean_min += 1  # Increment if there are duplicates\n",
    "                if n + i_mean_min < len(remainder):\n",
    "                    candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "                    replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "                    print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                    duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "            i_mean_min = i_mean_min+1\n",
    "\n",
    "\n",
    "    if asr_range < 2.0 and asr_mean_min < 0.5:\n",
    "        print(f\"Established group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "        # Assign the group number\n",
    "        candidate_group['Group Number'] = groupnumber\n",
    "        # Add the rows to the final list\n",
    "        grouped_list.append(candidate_group)\n",
    "        # Drop rows in \"remainder\" where \"Cell ID\" matches any in \"candidate_group\"\n",
    "        remainder = remainder[~remainder['Cell ID'].isin(candidate_group['Cell ID'])]\n",
    "        # Update processed indices\n",
    "        processed_indices.update(candidate_group.index)\n",
    "        # Update group number for the next group\n",
    "        groupnumber += 1\n",
    "        # Print status\n",
    "        print(f\"Rows in remainder after processing: {len(remainder)}\")\n",
    "\n",
    "\n",
    "# After loop, concatenate all the groups into a single DataFrame\n",
    "# Check if grouped_list contains valid DataFrames\n",
    "if not grouped_list:\n",
    "    print(\"No valid groups were formed. Creating an empty DataFrame.\")\n",
    "    df_final = pd.DataFrame()  # Handle case where no groups are created\n",
    "else:\n",
    "    df_final = pd.concat(grouped_list, ignore_index=True)\n",
    "# Add 'Remainder' label to remaining rows, if applicable\n",
    "if not remainder.empty:\n",
    "    remainder['Group Number'] = 'Remainder'\n",
    "    df_final = pd.concat([df_final, remainder], ignore_index=True)\n",
    "\n",
    "\n",
    "if not df_final.empty:\n",
    "    # Reset the index\n",
    "    df_final = df_final.reset_index(drop=True)    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['median_contour_catholyte_pct_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us', 'total_rank_score']\n",
    "    df_final = df_final.drop(columns=columns_to_drop, errors='ignore')  # Use 'errors=\"ignore\"' to avoid errors if columns are missing\n",
    "    # Move 'Group Number' to the first column\n",
    "    df_final = df_final[['Group Number'] + [col for col in df_final.columns if col != 'Group Number']]\n",
    "\n",
    "\n",
    "# Format final DataFrame\n",
    "#df_final = df_final.reset_index(drop=True)\n",
    "#df_final = df_final.drop(columns=['median_contour_catholyte_pct_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us']) #drop rows that we don't need to show in the spreadsheet\n",
    "#df_final = df_final[['Group Number'] + [col for col in df_final.columns if col != 'Group Number']] #move 'Group Number' to first column\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(f\"Final Tier 1 DataFrame has {df_final.shape[0]} rows.\")\n",
    "\n",
    "# Save the ranked cells to an Excel file\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_{batches}_Tier_1_Grouped.xlsx'\n",
    "df_final.to_excel(output_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 1: Arrange cells in each group by \"Pyramid\" order of ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rearrange rows in pyramid order within each group\n",
    "def pyramid_order(group):\n",
    "    # Sort values within the group\n",
    "    sorted_group = group.sort_values(by='MedDischargeASR_1C', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    # Rearrange in pyramid order\n",
    "    pyramid = []\n",
    "    while len(sorted_group) > 0:\n",
    "        if len(pyramid) % 2 == 0:  # Add largest remaining value to the middle\n",
    "            pyramid.append(sorted_group.iloc[-1])\n",
    "            sorted_group = sorted_group.iloc[:-1]\n",
    "        else:  # Add smallest remaining value to the sides\n",
    "            pyramid.insert(0, sorted_group.iloc[0])\n",
    "            sorted_group = sorted_group.iloc[1:]\n",
    "    \n",
    "    # Reconstruct DataFrame\n",
    "    pyramid_df = pd.DataFrame(pyramid).reset_index(drop=True)\n",
    "    \n",
    "    # Update Group Number to include position in group\n",
    "    pyramid_df['Group Number'] = group['Group Number'].iloc[0]  # Keep the group number\n",
    "    pyramid_df['Group Number'] = pyramid_df['Group Number'].astype(str) + \".\" + (pyramid_df.index + 1).astype(str)\n",
    "    \n",
    "    return pyramid_df\n",
    "\n",
    "if not df_final.empty:\n",
    "    # Apply the function to each group\n",
    "    df_pyramid = df_final.groupby('Group Number', group_keys=False).apply(pyramid_order)\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_pyramid = df_pyramid.reset_index(drop=True)\n",
    "\n",
    "    # Save the ranked cells to an Excel file\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    output_name = f'{current_date}_{batches}_Tier_1_Grouped_Pyramid_Order.xlsx'\n",
    "    df_pyramid.to_excel(output_name, index=False)\n",
    "else:\n",
    "    print(\"No Tier 1 dataframe to rearrange\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 2: Calculate Cell Score and Ranking using TOPSIS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scoring rules as dictionaries\n",
    "alignment_scores = {1: 0, 2: 0}  # 1->0, 2->0\n",
    "anode_scores = {1: 0, 2: 0.39}   # 1->0, 2->0.39\n",
    "thickness_scores = {1: 0, 2: 0}  # 1->0, 2->0\n",
    "edge_wetting_scores = {1: 0, 2: 0.06}  # 1->0, 2->0.06\n",
    "\n",
    "# Calculate individual scores for each column\n",
    "TieredTwoCells['AlignmentScore'] = TieredTwoCells['Alignment'].map(alignment_scores)\n",
    "TieredTwoCells['AnodeScore'] = TieredTwoCells['Anode'].map(anode_scores)\n",
    "TieredTwoCells['ThicknessScore'] = TieredTwoCells['Thickness'].map(thickness_scores)\n",
    "TieredTwoCells['EdgeWettingScore'] = TieredTwoCells['Edge Wetting'].map(edge_wetting_scores)\n",
    "\n",
    "# Calculate the total score\n",
    "TieredTwoCells['SumOfSpecPoints'] = (\n",
    "    TieredTwoCells['AlignmentScore'] +\n",
    "    TieredTwoCells['AnodeScore'] +\n",
    "    TieredTwoCells['ThicknessScore'] +\n",
    "    TieredTwoCells['EdgeWettingScore']\n",
    ")\n",
    "\n",
    "# Drop intermediate score columns (optional)\n",
    "TieredTwoCells = TieredTwoCells.drop(\n",
    "    ['AlignmentScore', 'AnodeScore', 'ThicknessScore', 'EdgeWettingScore'], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "##Calculate Cell Score and Ranking for Tier 2 Cells using TOPSIS method\n",
    "# Define the criteria and their corresponding optimization directions\n",
    "criteria = ['SumOfSpecPoints', 'MedDischargeASR_1C', 'DischargeCapacity_Co3'] \n",
    "weights = [0.70, 0.20, 0.10]  # Equal weights (adjust if needed)\n",
    "optimization_directions = ['Low', 'Low', 'High']  # 'Proximity' for 'Thickness'\n",
    "# Calculate TOPSIS scores\n",
    "TieredTwoCells['total_rank_score'] = topsis_with_target_Tier(TieredTwoCells, criteria, weights, optimization_directions, target_criteria, targets)\n",
    "\n",
    "# Rank alternatives by TOPSIS score (higher score is better)\n",
    "TieredTwoCells['Rank'] = TieredTwoCells['total_rank_score'].rank(ascending=False)\n",
    "# Arrange Cells by best ranking at the top\n",
    "TieredTwoCells.sort_values(by='Rank', ascending=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 2: Group Cells using ASR Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group starting at rank [1, 2, 3]: ['UCB003AK-US00-26', 'UCB003AM-US00-10', 'UCB003AN-US01-22']\n",
      "ASR range: 4.271700000000003, ASR mean-min: 1.4684333333333335\n",
      "ASR range is too high at 4.271700000000003, replacing problem cell: UCB003AM-US00-10\n",
      "Replacing UCB003AM-US00-10 with UCB003AN-US01-08\n",
      "ASR range: 0.2134999999999998, ASR mean-min: 0.11570000000000036\n",
      "Established group starting at rank [1, 4, 3]: ['UCB003AK-US00-26', 'UCB003AN-US01-08', 'UCB003AN-US01-22']\n",
      "Rows in remainder after processing: 2\n",
      "Final Tier 2 DataFrame has 5 rows.\n"
     ]
    }
   ],
   "source": [
    "df = TieredTwoCells\n",
    "df = df.reset_index(drop=True)\n",
    "df['Rank'] = (df.index + 1).astype(int)  # Add 1-based rank as integers\n",
    "remainder = df.copy()  # Start with a copy of the input DataFrame\n",
    "retries = 0  # Track retries for reprocessing the remainder\n",
    "\n",
    "n = Layers\n",
    "\n",
    "#Initialize variable and dataframe \n",
    "groupnumber = 1  # Initialize group number\n",
    "df_final_Two = pd.DataFrame()  # Initialize an empty DataFrame for the final result\n",
    "grouped_list = [] # List to collect all the groups for final concatenation\n",
    "cellstart = 0\n",
    "\n",
    "# Keep track of processed indices\n",
    "processed_indices = set()\n",
    "\n",
    "while len(remainder) >= n + cellstart:\n",
    "    # Find the next candidate group by slicing remainder\n",
    "    candidate_group = remainder.iloc[cellstart:cellstart+n].copy()\n",
    "    print(f\"Processing group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "\n",
    "    # Calculate criteria for the group\n",
    "    asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "    asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "\n",
    "    print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "\n",
    "    # Check if group satisfies criteria and replace cells if necessary\n",
    "    if asr_range >= 2.0:\n",
    "        # Replace the highest ASR cell\n",
    "        max_idx = candidate_group['MedDischargeASR_1C'].idxmax()\n",
    "        problem_cell_id = candidate_group.loc[max_idx, 'Cell ID']\n",
    "        print(f\"ASR range is too high at {asr_range}, replacing problem cell: {problem_cell_id}\")\n",
    "        # Replace the highest ASR cell here (you can replace it based on your logic)\n",
    "        i_range = 0\n",
    "\n",
    "        while asr_range >=2.0:\n",
    "            if n + i_range >= len(remainder):  # Check if the index is valid\n",
    "                print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                cellstart = cellstart +1\n",
    "                break\n",
    "            candidate_group.loc[max_idx] = remainder.iloc[n + i_range] # Replace the highest ASR cell\n",
    "            replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Cell ID of the replacement cell\n",
    "            print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "            # Check for duplicates in 'Cell ID' within the candidate group\n",
    "            duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "            while duplicate_count > 0:\n",
    "                print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_range.\")\n",
    "                i_range += 1  # Increment if there are duplicates\n",
    "                if n + i_range < len(remainder):\n",
    "                    candidate_group.loc[max_idx] = remainder.iloc[n + i_range]\n",
    "                    replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Replacement cell ID\n",
    "                    print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                    duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                else:\n",
    "                    break\n",
    "            asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "            i_range = i_range+1\n",
    "\n",
    "    if asr_mean_min >= 0.5:\n",
    "        # Replace the lowest ASR cell\n",
    "        min_idx = candidate_group['MedDischargeASR_1C'].idxmin()\n",
    "        problem_cell_id = candidate_group.loc[min_idx, 'Cell ID']\n",
    "        print(f\"Minimum ASR is too low at difference of {asr_mean_min}, replacing problem cell: {problem_cell_id}\")\n",
    "        # Replace the lowest ASR cell here (you can replace it based on your logic)\n",
    "        i_mean_min = 0\n",
    "\n",
    "        while asr_mean_min >= 0.5:\n",
    "            if n + i_mean_min >= len(remainder):  # Check if the index is valid\n",
    "                print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                cellstart = cellstart +1\n",
    "                break\n",
    "            candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "            replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "            print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "            # Check for duplicates in 'Cell ID' within the candidate group\n",
    "            duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "            while duplicate_count > 0:\n",
    "                print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_mean_min.\")\n",
    "                i_mean_min += 1  # Increment if there are duplicates\n",
    "                if n + i_mean_min < len(remainder):\n",
    "                    candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "                    replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "                    print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                    duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "            i_mean_min = i_mean_min+1\n",
    "\n",
    "\n",
    "    if asr_range < 2.0 and asr_mean_min < 0.5:\n",
    "        print(f\"Established group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "        # Assign the group number\n",
    "        candidate_group['Group Number'] = groupnumber\n",
    "        # Add the rows to the final list\n",
    "        grouped_list.append(candidate_group)\n",
    "        # Drop rows in \"remainder\" where \"Cell ID\" matches any in \"candidate_group\"\n",
    "        remainder = remainder[~remainder['Cell ID'].isin(candidate_group['Cell ID'])]\n",
    "        # Update processed indices\n",
    "        processed_indices.update(candidate_group.index)\n",
    "        # Update group number for the next group\n",
    "        groupnumber += 1\n",
    "        # Print status\n",
    "        print(f\"Rows in remainder after processing: {len(remainder)}\")\n",
    "\n",
    "\n",
    "# After loop, concatenate all the groups into a single DataFrame\n",
    "# Check if grouped_list contains valid DataFrames\n",
    "if not grouped_list:\n",
    "    print(\"No valid groups were formed. Creating an empty DataFrame.\")\n",
    "    df_final_Two = pd.DataFrame()  # Handle case where no groups are created\n",
    "else:\n",
    "    df_final_Two = pd.concat(grouped_list, ignore_index=True)\n",
    "# Add 'Remainder' label to remaining rows, if applicable\n",
    "if not remainder.empty:\n",
    "    remainder['Group Number'] = 'Remainder'\n",
    "    df_final_Two = pd.concat([df_final_Two, remainder], ignore_index=True)\n",
    "\n",
    "\n",
    "if not df_final_Two.empty:\n",
    "    # Reset the index\n",
    "    df_final_Two = df_final_Two.reset_index(drop=True)    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['median_contour_catholyte_pct_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us', 'SumOfSpecPoints', 'total_rank_score']\n",
    "    df_final_Two = df_final_Two.drop(columns=columns_to_drop, errors='ignore')  # Use 'errors=\"ignore\"' to avoid errors if columns are missing\n",
    "    # Move 'Group Number' to the first column\n",
    "    df_final_Two = df_final_Two[['Group Number'] + [col for col in df_final_Two.columns if col != 'Group Number']]\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(f\"Final Tier 2 DataFrame has {df_final_Two.shape[0]} rows.\")\n",
    "\n",
    "# Save the ranked cells to an Excel file\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_{batches}_Tier_2_Grouped.xlsx'\n",
    "df_final_Two.to_excel(output_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 2: Arrange cells in each group by \"Pyramid\" order of ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rearrange rows in pyramid order\n",
    "if not df_final_Two.empty:\n",
    "    # Apply the function to each group\n",
    "    df_pyramid_Two = df_final_Two.groupby('Group Number', group_keys=False).apply(pyramid_order)\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_pyramid_Two = df_pyramid_Two.reset_index(drop=True)\n",
    "\n",
    "    # Save the ranked cells to an Excel file\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    output_name = f'{current_date}_{batches}_Tier_2_Grouped_Pyramid_Order.xlsx'\n",
    "    df_pyramid_Two.to_excel(output_name, index=False)\n",
    "else:\n",
    "    print(\"No Tier 2 dataframe to rearrange\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 3: Calculate Cell Score and Ranking using TOPSIS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scoring rules as dictionaries\n",
    "alignment_scores = {1: 0, 2: 0, 3: 0.26}  # scores for Alignment\n",
    "anode_scores = {1: 0, 2: 0.39, 3: 0.70}   # scores for Anode\n",
    "thickness_scores = {1: 0, 2: 0, 3: 0.33}  # scores for Thickness\n",
    "edge_wetting_scores = {1: 0, 2: 0.06, 3: 0.26}  # scores for EdgeWetting\n",
    "\n",
    "# Calculate individual scores for each column\n",
    "TieredThreeCells['AlignmentScore'] = TieredThreeCells['Alignment'].map(alignment_scores)\n",
    "TieredThreeCells['AnodeScore'] = TieredThreeCells['Anode'].map(anode_scores)\n",
    "TieredThreeCells['ThicknessScore'] = TieredThreeCells['Thickness'].map(thickness_scores)\n",
    "TieredThreeCells['EdgeWettingScore'] = TieredThreeCells['Edge Wetting'].map(edge_wetting_scores)\n",
    "\n",
    "# Calculate the total score\n",
    "TieredThreeCells['SumOfSpecPoints'] = (\n",
    "    TieredThreeCells['AlignmentScore'] +\n",
    "    TieredThreeCells['AnodeScore'] +\n",
    "    TieredThreeCells['ThicknessScore'] +\n",
    "    TieredThreeCells['EdgeWettingScore']\n",
    ")\n",
    "\n",
    "# Drop intermediate score columns (optional)\n",
    "TieredThreeCells = TieredThreeCells.drop(\n",
    "    ['AlignmentScore', 'AnodeScore', 'ThicknessScore', 'EdgeWettingScore'], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "##Calculate Cell Score and Ranking for Tier 2 Cells using TOPSIS method\n",
    "# Define the criteria and their corresponding optimization directions\n",
    "criteria = ['SumOfSpecPoints', 'MedDischargeASR_1C', 'DischargeCapacity_Co3'] \n",
    "weights = [0.70, 0.20, 0.10]  # Equal weights (adjust if needed)\n",
    "optimization_directions = ['Low', 'Low', 'High']  # 'Proximity' for 'Thickness'\n",
    "# Calculate TOPSIS scores\n",
    "TieredThreeCells['total_rank_score'] = topsis_with_target_Tier(TieredThreeCells, criteria, weights, optimization_directions, target_criteria, targets)\n",
    "\n",
    "# Rank alternatives by TOPSIS score (higher score is better)\n",
    "TieredThreeCells['Rank'] = TieredThreeCells['total_rank_score'].rank(ascending=False)\n",
    "# Arrange Cells by best ranking at the top\n",
    "TieredThreeCells.sort_values(by='Rank', ascending=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 3: Group Cells using ASR Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group starting at rank [1, 2, 3]: ['UCB003AN-US00-13', 'UCB003AN-US00-21', 'UCB003AN-US00-08']\n",
      "ASR range: 0.9673000000000016, ASR mean-min: 0.4542333333333346\n",
      "Established group starting at rank [1, 2, 3]: ['UCB003AN-US00-13', 'UCB003AN-US00-21', 'UCB003AN-US00-08']\n",
      "Rows in remainder after processing: 22\n",
      "Processing group starting at rank [4, 5, 6]: ['UCB003AL-US00-42', 'UCB003AN-US00-01', 'UCB003AL-US00-19']\n",
      "ASR range: 0.18459999999999965, ASR mean-min: 0.09219999999999828\n",
      "Established group starting at rank [4, 5, 6]: ['UCB003AL-US00-42', 'UCB003AN-US00-01', 'UCB003AL-US00-19']\n",
      "Rows in remainder after processing: 19\n",
      "Processing group starting at rank [7, 8, 9]: ['UCB003AL-US00-38', 'UCB003AK-US00-10', 'UCB003AL-US00-33']\n",
      "ASR range: 0.4576000000000029, ASR mean-min: 0.3044000000000011\n",
      "Established group starting at rank [7, 8, 9]: ['UCB003AL-US00-38', 'UCB003AK-US00-10', 'UCB003AL-US00-33']\n",
      "Rows in remainder after processing: 16\n",
      "Processing group starting at rank [10, 11, 12]: ['UCB003AL-US00-75', 'UCB003AL-US00-66', 'UCB003AL-US00-46']\n",
      "ASR range: 0.35050000000000026, ASR mean-min: 0.19709999999999894\n",
      "Established group starting at rank [10, 11, 12]: ['UCB003AL-US00-75', 'UCB003AL-US00-66', 'UCB003AL-US00-46']\n",
      "Rows in remainder after processing: 13\n",
      "Processing group starting at rank [13, 14, 15]: ['UCB003AL-US00-73', 'UCB003AN-US00-14', 'UCB003AM-US00-32']\n",
      "ASR range: 2.7179, ASR mean-min: 0.9812333333333321\n",
      "ASR range is too high at 2.7179, replacing problem cell: UCB003AL-US00-73\n",
      "Replacing UCB003AL-US00-73 with UCB003AL-US00-13\n",
      "ASR range: 0.40419999999999945, ASR mean-min: 0.21000000000000085\n",
      "Established group starting at rank [16, 14, 15]: ['UCB003AL-US00-13', 'UCB003AN-US00-14', 'UCB003AM-US00-32']\n",
      "Rows in remainder after processing: 10\n",
      "Processing group starting at rank [13, 17, 18]: ['UCB003AL-US00-73', 'UCB003AK-US01-10', 'UCB003AN-US01-27']\n",
      "ASR range: 2.197800000000001, ASR mean-min: 0.8012333333333359\n",
      "ASR range is too high at 2.197800000000001, replacing problem cell: UCB003AL-US00-73\n",
      "Replacing UCB003AL-US00-73 with UCB003AL-US00-22\n",
      "ASR range: 0.7264999999999979, ASR mean-min: 0.31079999999999686\n",
      "Established group starting at rank [19, 17, 18]: ['UCB003AL-US00-22', 'UCB003AK-US01-10', 'UCB003AN-US01-27']\n",
      "Rows in remainder after processing: 7\n",
      "Processing group starting at rank [13, 20, 21]: ['UCB003AL-US00-73', 'UCB003AL-US00-49', 'UCB003AL-US00-31']\n",
      "ASR range: 1.7067000000000014, ASR mean-min: 1.089299999999998\n",
      "Minimum ASR is too low at difference of 1.089299999999998, replacing problem cell: UCB003AL-US00-31\n",
      "Replacing UCB003AL-US00-31 with UCB003AL-US00-20\n",
      "ASR range: 1.5339000000000027, ASR mean-min: 0.9741\n",
      "Replacing UCB003AL-US00-31 with UCB003AN-US01-03\n",
      "ASR range: 2.6656000000000013, ASR mean-min: 1.7285666666666657\n",
      "Replacing UCB003AL-US00-31 with UCB003AK-US00-12\n",
      "ASR range: 2.0366, ASR mean-min: 0.7273666666666649\n",
      "Replacing UCB003AL-US00-31 with UCB003AN-US01-32\n",
      "ASR range: 1.7410000000000032, ASR mean-min: 1.112166666666667\n",
      "No more rows in remainder to replace UCB003AL-US00-31.\n",
      "Processing group starting at rank [20, 21, 22]: ['UCB003AL-US00-49', 'UCB003AL-US00-31', 'UCB003AL-US00-20']\n",
      "ASR range: 1.5611999999999995, ASR mean-min: 0.5779999999999994\n",
      "Minimum ASR is too low at difference of 0.5779999999999994, replacing problem cell: UCB003AL-US00-31\n",
      "Replacing UCB003AL-US00-31 with UCB003AL-US00-20\n",
      "Found 2 duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_mean_min.\n",
      "Replacing UCB003AL-US00-31 with UCB003AN-US01-03\n",
      "ASR range: 2.5200999999999993, ASR mean-min: 1.2172666666666636\n",
      "Replacing UCB003AL-US00-31 with UCB003AK-US00-12\n",
      "ASR range: 3.4250000000000007, ASR mean-min: 1.6044666666666672\n",
      "Replacing UCB003AL-US00-31 with UCB003AN-US01-32\n",
      "ASR range: 1.5955000000000013, ASR mean-min: 0.6008666666666649\n",
      "No more rows in remainder to replace UCB003AL-US00-31.\n",
      "Processing group starting at rank [21, 22, 23]: ['UCB003AL-US00-31', 'UCB003AL-US00-20', 'UCB003AN-US01-03']\n",
      "ASR range: 1.1316999999999986, ASR mean-min: 0.696866666666665\n",
      "Minimum ASR is too low at difference of 0.696866666666665, replacing problem cell: UCB003AN-US01-03\n",
      "Replacing UCB003AN-US01-03 with UCB003AL-US00-20\n",
      "Found 2 duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_mean_min.\n",
      "Replacing UCB003AN-US01-03 with UCB003AN-US01-03\n",
      "ASR range: 1.1316999999999986, ASR mean-min: 0.696866666666665\n",
      "Replacing UCB003AN-US01-03 with UCB003AK-US00-12\n",
      "ASR range: 3.5977999999999994, ASR mean-min: 1.2568666666666637\n",
      "Replacing UCB003AN-US01-03 with UCB003AN-US01-32\n",
      "ASR range: 0.2071000000000005, ASR mean-min: 0.08046666666666624\n",
      "Established group starting at rank [21, 22, 25]: ['UCB003AL-US00-31', 'UCB003AL-US00-20', 'UCB003AN-US01-32']\n",
      "Rows in remainder after processing: 4\n",
      "Final Tier 2 DataFrame has 25 rows.\n"
     ]
    }
   ],
   "source": [
    "df = TieredThreeCells\n",
    "df = df.reset_index(drop=True)\n",
    "df['Rank'] = (df.index + 1).astype(int)  # Add 1-based rank as integers\n",
    "remainder = df.copy()  # Start with a copy of the input DataFrame\n",
    "retries = 0  # Track retries for reprocessing the remainder\n",
    "\n",
    "n = Layers\n",
    "\n",
    "#Initialize variable and dataframe \n",
    "groupnumber = 1  # Initialize group number\n",
    "df_final_Three = pd.DataFrame()  # Initialize an empty DataFrame for the final result\n",
    "grouped_list = [] # List to collect all the groups for final concatenation\n",
    "cellstart = 0\n",
    "\n",
    "# Keep track of processed indices\n",
    "processed_indices = set()\n",
    "\n",
    "while len(remainder) >= n + cellstart:\n",
    "    # Find the next candidate group by slicing remainder\n",
    "    candidate_group = remainder.iloc[cellstart:cellstart+n].copy()\n",
    "    print(f\"Processing group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "\n",
    "    # Calculate criteria for the group\n",
    "    asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "    asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "\n",
    "    print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "\n",
    "    # Check if group satisfies criteria and replace cells if necessary\n",
    "    if asr_range >= 2.0:\n",
    "        # Replace the highest ASR cell\n",
    "        max_idx = candidate_group['MedDischargeASR_1C'].idxmax()\n",
    "        problem_cell_id = candidate_group.loc[max_idx, 'Cell ID']\n",
    "        print(f\"ASR range is too high at {asr_range}, replacing problem cell: {problem_cell_id}\")\n",
    "        # Replace the highest ASR cell here (you can replace it based on your logic)\n",
    "        i_range = 0\n",
    "\n",
    "        while asr_range >=2.0:\n",
    "            if n + i_range >= len(remainder):  # Check if the index is valid\n",
    "                print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                cellstart = cellstart +1\n",
    "                break\n",
    "            candidate_group.loc[max_idx] = remainder.iloc[n + i_range] # Replace the highest ASR cell\n",
    "            replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Cell ID of the replacement cell\n",
    "            print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "            # Check for duplicates in 'Cell ID' within the candidate group\n",
    "            duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "            while duplicate_count > 0:\n",
    "                print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_range.\")\n",
    "                i_range += 1  # Increment if there are duplicates\n",
    "                if n + i_range < len(remainder):\n",
    "                    candidate_group.loc[max_idx] = remainder.iloc[n + i_range]\n",
    "                    replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Replacement cell ID\n",
    "                    print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                    duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                else:\n",
    "                    break\n",
    "            asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "            i_range = i_range+1\n",
    "\n",
    "    if asr_mean_min >= 0.5:\n",
    "        # Replace the lowest ASR cell\n",
    "        min_idx = candidate_group['MedDischargeASR_1C'].idxmin()\n",
    "        problem_cell_id = candidate_group.loc[min_idx, 'Cell ID']\n",
    "        print(f\"Minimum ASR is too low at difference of {asr_mean_min}, replacing problem cell: {problem_cell_id}\")\n",
    "        # Replace the lowest ASR cell here (you can replace it based on your logic)\n",
    "        i_mean_min = 0\n",
    "\n",
    "        while asr_mean_min >= 0.5:\n",
    "            if n + i_mean_min >= len(remainder):  # Check if the index is valid\n",
    "                print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                cellstart = cellstart +1\n",
    "                break\n",
    "            candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "            replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "            print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "            # Check for duplicates in 'Cell ID' within the candidate group\n",
    "            duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "            while duplicate_count > 0:\n",
    "                print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_mean_min.\")\n",
    "                i_mean_min += 1  # Increment if there are duplicates\n",
    "                if n + i_mean_min < len(remainder):\n",
    "                    candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "                    replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "                    print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                    duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "            i_mean_min = i_mean_min+1\n",
    "\n",
    "\n",
    "    if asr_range < 2.0 and asr_mean_min < 0.5:\n",
    "        print(f\"Established group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "        # Assign the group number\n",
    "        candidate_group['Group Number'] = groupnumber\n",
    "        # Add the rows to the final list\n",
    "        grouped_list.append(candidate_group)\n",
    "        # Drop rows in \"remainder\" where \"Cell ID\" matches any in \"candidate_group\"\n",
    "        remainder = remainder[~remainder['Cell ID'].isin(candidate_group['Cell ID'])]\n",
    "        # Update processed indices\n",
    "        processed_indices.update(candidate_group.index)\n",
    "        # Update group number for the next group\n",
    "        groupnumber += 1\n",
    "        # Print status\n",
    "        print(f\"Rows in remainder after processing: {len(remainder)}\")\n",
    "\n",
    "\n",
    "# After loop, concatenate all the groups into a single DataFrame\n",
    "# Check if grouped_list contains valid DataFrames\n",
    "if not grouped_list:\n",
    "    print(\"No valid groups were formed. Creating an empty DataFrame.\")\n",
    "    df_final_Three = pd.DataFrame()  # Handle case where no groups are created\n",
    "else:\n",
    "    df_final_Three = pd.concat(grouped_list, ignore_index=True)\n",
    "# Add 'Remainder' label to remaining rows, if applicable\n",
    "if not remainder.empty:\n",
    "    remainder['Group Number'] = 'Remainder'\n",
    "    df_final_Three = pd.concat([df_final_Three, remainder], ignore_index=True)\n",
    "\n",
    "\n",
    "if not df_final_Three.empty:\n",
    "    # Reset the index\n",
    "    df_final_Three = df_final_Three.reset_index(drop=True)    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['median_contour_catholyte_pct_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us', 'SumOfSpecPoints', 'total_rank_score']\n",
    "    df_final_Three = df_final_Three.drop(columns=columns_to_drop, errors='ignore')  # Use 'errors=\"ignore\"' to avoid errors if columns are missing\n",
    "    # Move 'Group Number' to the first column\n",
    "    df_final_Three = df_final_Three[['Group Number'] + [col for col in df_final_Three.columns if col != 'Group Number']]\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(f\"Final Tier 2 DataFrame has {df_final_Three.shape[0]} rows.\")\n",
    "\n",
    "# Save the ranked cells to an Excel file\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_{batches}_Tier_3_Grouped.xlsx'\n",
    "df_final_Three.to_excel(output_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 3: Arrange cells in each group by \"Pyramid\" order of ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rearrange rows in pyramid order\n",
    "if not df_final_Three.empty:\n",
    "    # Apply the function to each group\n",
    "    df_pyramid_Three = df_final_Three.groupby('Group Number', group_keys=False).apply(pyramid_order)\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_pyramid_Three = df_pyramid_Three.reset_index(drop=True)\n",
    "\n",
    "    # Save the ranked cells to an Excel file\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    output_name = f'{current_date}_{batches}_Tier_3_Grouped_Pyramid_Order.xlsx'\n",
    "    df_pyramid_Three.to_excel(output_name, index=False)\n",
    "else:\n",
    "    print(\"No Tier 3 dataframe to rearrange\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
