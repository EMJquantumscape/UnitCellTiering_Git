{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "c:\\Users\\EDM02\\OneDrive - QuantumScape Corporation\\Files\\Python Scripts\\UnitCellTiering_Git\\Archive\\genealogy_v2.py:533: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pivoted_df[column_name] = None\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "# %% Import Modules\n",
    "######## Import Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick  # Importing matplotlib ticker module for formatting\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from itertools import groupby\n",
    "from lifelines import KaplanMeierFitter\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "import plotly\n",
    "pio.templates.default = \"plotly_white\"\n",
    "clrs = plotly.colors.DEFAULT_PLOTLY_COLORS\n",
    "from qsdc.client import Client\n",
    "import met_client as app\n",
    "import genealogy\n",
    "import genealogy_v2\n",
    "import unit_cell_electrical_yield_and_metrics_with_rel as uceym_rel\n",
    "import unit_cell_metro_metrics\n",
    "import cell_tiering_metro\n",
    "import mass\n",
    "from qsdc.client import Client\n",
    "# create the quantumscape data client\n",
    "qs_client = Client()\n",
    "# Query data\n",
    "conn = qs_client.get_mysql_engine()\n",
    "\n",
    "batches = \"UCD003A[A-E]|UCD004A[F-G]|UCD005A[A-C]|UCD006A[A-B]|UCD006A[D-E]|UCD006A[G-H]|UCD006A[J-N]|UCD008AA|UCD011A[A-E]|UCD005AF|UCD006A[R-S]|UCD013AA\" #UCD mastertracker\n",
    "batches = \"UCB003A[K-N]\"#|UCB003AE-US00-24|UCB003AF-US00-24|UCB003AF-US00-26|UCB003AG-US00-02|UCB003AG-US00-15|UCB003AG-US00-34|UCB003AG-US00-35|\"\n",
    "batches = \"UCB004A\"\n",
    "#batches = \"UCB003AN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "# %% Query Data\n",
    "######## Query Data\n",
    "# =============================================================================\n",
    "# ========================         QUERY DATA          =======================\n",
    "# =============================================================================\n",
    "\n",
    "######## CELL GENEALOGY ########\n",
    "# produce dataframe that lists all cells in the batches specified above \n",
    "df_sample_ids = genealogy.get_unit_cells(batches) # query ID's of unit cells that are in the batches mentioned above\n",
    "\n",
    "# creates dataframe with the genealogy history of each unit cell\n",
    "df_genealogy = genealogy.get_genealogy_unitcell(df_sample_ids[\"US_id\"]) # queries the genealogy history of every sell in the batch\n",
    "df_samples = df_sample_ids.merge(df_genealogy, on=\"US_id\", how=\"left\") # merges the list of unit cells with the geneology dataframe \n",
    "\n",
    "\n",
    "######## QUERY ELECTRICAL YIELD AND ELECTRICAL METRICS ########\n",
    "# get yield and electrical metrics for each unit cells\n",
    "df_electrical_yield_metrics = uceym_rel.get_electrical_yield_and_metrics(df_samples[\"US_id\"])\n",
    "df_testdata = df_samples.merge(df_electrical_yield_metrics, on=\"US_id\") #merges this list with the previous data frame\n",
    "df_testdata = df_testdata.sort_values('US_id')\n",
    "\n",
    "\n",
    "######## METROLOGY DATA \n",
    "agent = app.ImageAgent()\n",
    "# get anode metrics from hifi scds scans\n",
    "anode_metrics = unit_cell_metro_metrics.get_anode_tier(\n",
    "    df_testdata[\"US_id\"].str.slice(stop=13).unique(), agent\n",
    ")\n",
    "anode_metrics.rename(columns={\"sample\": \"US_id\"}, inplace=True)\n",
    "# get radiograph metrics from nordson matrix x-ray scans\n",
    "radiograph_metrics = unit_cell_metro_metrics.get_radiograph_tier(\n",
    "    df_testdata[\"US_id\"].str.slice(stop=13).unique(), agent\n",
    ")\n",
    "radiograph_metrics.rename(columns={\"sample\": \"US_id\"}, inplace=True)\n",
    "# get unit cell mass metrics\n",
    "mass_metrics = mass.get_mass_data(df_testdata[\"US_id\"])\n",
    "# unit cell tiering (alpha 2 criteria)\n",
    "df_testdata = cell_tiering_metro.cell_tiering(\n",
    "    df_testdata, anode_metrics, radiograph_metrics, mass_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot Screen UCT Yields\n",
    "######## Plot Screen UCT Yields\n",
    "#  =============================================================================\n",
    "# ========================      SCREEN UCT YIELDS        =======================\n",
    "# =============================================================================\n",
    "\n",
    "##Take dataframe for individual cell and group them by batch (\"process\")\n",
    "grouping = \"process\"\n",
    "data = df_testdata.copy() #make copy\n",
    "\n",
    "\n",
    "#data = data[~data['US_id'].str.contains(\"APD224A|APD238AE\")] #focus on subset by filtering out\n",
    "\n",
    "#create new dataframe where cells from the same batches are grouped together\n",
    "df_cyield = (\n",
    "    data[[grouping]]\n",
    "    .groupby(grouping)\n",
    "    .first()\n",
    "    .join(\n",
    "        data[\n",
    "            [\n",
    "                grouping,\n",
    "                \"Build Count\",\n",
    "                \"Formation Count\",\n",
    "                \"C/3 Count\",\n",
    "                \"1C Count\",\n",
    "                \"Fast Charge Count\",\n",
    "                \"4C Count\",\n",
    "                \"Final 1C Count\",\n",
    "                \"Yield Count\",\n",
    "                # \"Anode Count\",\n",
    "                # \"Anode Yield Count\",\n",
    "                # \"Anode Yield Tier 1 Count\",\n",
    "                # \"Ultrasound Count\",\n",
    "                # \"Ultrasound Yield Count\",\n",
    "                # \"Metrology Count\",\n",
    "                # \"Tier 1 Count\",\n",
    "                # \"Tier 1+2 Count\",\n",
    "            ]\n",
    "        ]\n",
    "        .groupby(grouping)\n",
    "        .sum(),\n",
    "        how=\"right\",\n",
    "    )\n",
    "    .reset_index()\n",
    ").set_index(grouping)\n",
    "\n",
    "\n",
    "df_cyield[\n",
    "    [\n",
    "        \"Cells Built\",\n",
    "        \"Formation Yield\",\n",
    "        \"C/3 Yield\",\n",
    "        \"1C Yield\",\n",
    "        \"Fast Charge Yield\",\n",
    "        \"4C Yield\",\n",
    "        \"Final 1C Yield\",\n",
    "        \"Screen Yield\",\n",
    "    ]\n",
    "] = 100 * df_cyield[\n",
    "    [\n",
    "        \"Build Count\",\n",
    "        \"Formation Count\",\n",
    "        \"C/3 Count\",\n",
    "        \"1C Count\",\n",
    "        \"Fast Charge Count\",\n",
    "        \"4C Count\",\n",
    "        \"Final 1C Count\",\n",
    "        \"Yield Count\",\n",
    "    ]\n",
    "].div(\n",
    "    df_cyield[\"Build Count\"], axis=0\n",
    ")\n",
    "\n",
    "# df_cyield[\"Anode Yield Tier 1+2\"] = (\n",
    "#     df_cyield[\"Anode Yield Count\"] / df_cyield[\"Anode Count\"]\n",
    "# ) * df_cyield[\"Screen Yield\"]\n",
    "\n",
    "# df_cyield[\"Anode Yield Tier 1\"] = (\n",
    "#     df_cyield[\"Anode Yield Tier 1 Count\"] / df_cyield[\"Anode Count\"]\n",
    "# ) * df_cyield[\"Screen Yield\"]\n",
    "\n",
    "# df_cyield[\"Tier 1+2 Yield\"] = (\n",
    "#     df_cyield[\"Tier 1+2 Count\"] / df_cyield[\"Metrology Count\"]\n",
    "# ) * df_cyield[\"Screen Yield\"]\n",
    "\n",
    "# df_cyield[\"Tier 1 Yield\"] = (\n",
    "#     df_cyield[\"Tier 1 Count\"] / df_cyield[\"Metrology Count\"]\n",
    "# ) * df_cyield[\"Screen Yield\"]\n",
    "\n",
    "\n",
    "df_cyield = df_cyield.sort_values(grouping)\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Bar(\n",
    "            x=df_cyield.index,\n",
    "            y=df_cyield[\"Cells Built\"],\n",
    "            name=\"Cells Built\",\n",
    "        ),\n",
    "        go.Bar(\n",
    "            x=df_cyield.index,\n",
    "            y=df_cyield[\"Formation Yield\"],\n",
    "            name=\"Formation Yield\",\n",
    "        ),\n",
    "        go.Bar(\n",
    "            x=df_cyield.index,\n",
    "            y=df_cyield[\"1C Yield\"],\n",
    "            name=\"1C Yield\",\n",
    "        ),\n",
    "        go.Bar(\n",
    "            x=df_cyield.index,\n",
    "            y=df_cyield[\"Fast Charge Yield\"],\n",
    "            name=\"2.5C Yield\",\n",
    "        ),\n",
    "        # go.Bar(\n",
    "        #     x=df_cyield.index,\n",
    "        #     y=df_cyield[\"4C Yield\"],\n",
    "        #     name=\"4C Yield\",\n",
    "        # ),\n",
    "        go.Bar(\n",
    "            x=df_cyield.index,\n",
    "            y=df_cyield[\"Final 1C Yield\"],\n",
    "            name=\"Final 1C Yield\",\n",
    "        ),\n",
    "        go.Bar(\n",
    "            x=df_cyield.index,\n",
    "            y=df_cyield[\"C/3 Yield\"],\n",
    "            name=\"C/3 Yield\",\n",
    "        ),\n",
    "        # go.Bar(\n",
    "        #     x=df_cyield.index,\n",
    "        #     y=df_cyield[\"Anode Yield Tier 1+2\"],\n",
    "        #     name=\"Anode 1+2 Yield\",  # (HiFi, Ultran, Mass, Radiograph)\",\n",
    "        # ),\n",
    "    #     go.Bar(\n",
    "    #         x=df_cyield.index,\n",
    "    #         y=df_cyield[\"Anode Yield Tier 1\"],\n",
    "    #         name=\"Anode Yield (Tier 1)\",  # (HiFi, Ultran, Mass, Radiograph)\",\n",
    "    #     ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# bar group mode\n",
    "fig.update_layout(barmode=\"group\")\n",
    "\n",
    "fig.data[0].text = [f\"N= {n}\" for n in df_cyield[\"Build Count\"]]\n",
    "fig.update_traces(textposition=\"inside\", textfont_size=15)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=grouping,\n",
    "    yaxis_title=\"Screen yield (%)\",\n",
    "    # y axis font size = 14, x axis font size = 11\n",
    "    font=dict(size=14),\n",
    "    legend={\"title_text\": \"\"},\n",
    "    yaxis_range=[0, 101],\n",
    ")\n",
    "\n",
    "\n",
    "# fig.update_xaxes(\n",
    "#     categoryorder=\"array\",\n",
    "#     categoryarray=data.sort_values([\"cell_build_date\", \"batch\"])[grouping].unique(),\n",
    "# )\n",
    "\n",
    "fig.update_xaxes(\n",
    "    categoryorder=\"array\",\n",
    "    categoryarray=data.sort_values([\"batch\"])[grouping].unique(),\n",
    ")\n",
    "\n",
    "\n",
    "# change the bar colors\n",
    "colors = [\n",
    "    px.colors.qualitative.Plotly[2],\n",
    "    px.colors.qualitative.Plotly[5],\n",
    "    px.colors.qualitative.Plotly[6],\n",
    "    px.colors.qualitative.Plotly[4],\n",
    "    # px.colors.qualitative.Plotly[7],\n",
    "    px.colors.qualitative.Plotly[0],\n",
    "    px.colors.qualitative.Plotly[3],\n",
    "    px.colors.qualitative.Plotly[1],\n",
    "]\n",
    "for i in range(len(fig.data)):\n",
    "    fig.data[i].marker.color = colors[i]\n",
    "\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot Cell Metrics\n",
    "######## Plot Cell Metrics\n",
    "# =============================================================================\n",
    "# ========================        CELL METRICS          =======================\n",
    "# =============================================================================\n",
    "\n",
    "grouping = \"process\"\n",
    "color_by = \"experiment\"\n",
    "\n",
    "data = df_testdata.copy()\n",
    "\n",
    "\n",
    "fig = make_subplots(\n",
    "    1,\n",
    "    2,\n",
    "    horizontal_spacing=0.12,\n",
    "    vertical_spacing=0.1,\n",
    "    shared_xaxes=True,\n",
    ")\n",
    "\n",
    "# create a color dictionary for each color_by category\n",
    "color = dict(zip(data[color_by].unique(), px.colors.qualitative.Plotly))\n",
    "\n",
    "# Set a flag to ensure legend items are added only once\n",
    "legend_added = {key: False for key in data[color_by].unique()}\n",
    "\n",
    "for label, group in data.groupby(grouping):\n",
    "    for color_value, group_color in group.groupby(color_by):\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                x=group_color[grouping],\n",
    "                y=group_color[\"AMSDischargeCapactiy_Co3\"],\n",
    "                quartilemethod=\"linear\",\n",
    "                name=color_value,\n",
    "                text=group_color[\"US_id\"],\n",
    "                showlegend=not legend_added[color_value],\n",
    "                fillcolor=color[color_value],\n",
    "                line=dict(color=\"black\"),\n",
    "            ),\n",
    "            1,\n",
    "            1,\n",
    "        )\n",
    "        legend_added[color_value] = True\n",
    "\n",
    "\n",
    "fig.update_yaxes(\n",
    "    title_text=\"1C Discharge Capacity (mAh/g)\",\n",
    "    range=[190, 205],\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "for label, group in data.groupby(grouping):\n",
    "    for color_value, group_color in group.groupby(color_by):\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                x=group_color[grouping],\n",
    "                y=group_color[\n",
    "                    \"ADDischargeCapactiy_Co3\"\n",
    "                ],  # [group[\"Final 1C Count\"] == 1]\n",
    "                quartilemethod=\"linear\",\n",
    "                name=color_value,\n",
    "                text=group_color[\"US_id\"],\n",
    "                showlegend=not legend_added[color_value],\n",
    "                fillcolor=color[color_value],\n",
    "                line=dict(color=\"black\"),\n",
    "            ),\n",
    "            1,\n",
    "            2,\n",
    "        )\n",
    "        legend_added[color_value] = True\n",
    "\n",
    "\n",
    "fig.update_yaxes(\n",
    "    title_text=\"C/3 Discharge Capacity (mAh/cm<sup>2</sup>)\",\n",
    "    range=[5, 7],\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "for i in range(2):\n",
    "    fig.update_yaxes(\n",
    "        showline=True,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=1,\n",
    "        mirror=True,\n",
    "        ticks=\"outside\",\n",
    "        row=1,\n",
    "        col=i + 1,\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        showline=True,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=1,\n",
    "        mirror=True,\n",
    "        ticks=\"outside\",\n",
    "        row=1,\n",
    "        col=i + 1,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"\",\n",
    "    # xaxis_title=grouping,\n",
    "    font=dict(\n",
    "        size=16,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.update_traces(boxpoints=\"all\", jitter=0.1)\n",
    "\n",
    "fig.update_xaxes(\n",
    "    categoryorder=\"array\",\n",
    "    categoryarray=data.sort_values([\"cell_build_date\", \"batch\"])[grouping].unique(),\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"browser\")\n",
    "\n",
    "\n",
    "fig = make_subplots(\n",
    "    1,\n",
    "    2,\n",
    "    horizontal_spacing=0.12,\n",
    "    vertical_spacing=0.1,\n",
    "    shared_xaxes=True,\n",
    ")\n",
    "\n",
    "# plot colors in px.colors.qualitative.Plotly\n",
    "color = dict(zip(data[color_by].unique(), px.colors.qualitative.Plotly))\n",
    "\n",
    "\n",
    "# Set a flag to ensure legend items are added only once\n",
    "legend_added = {key: False for key in data[color_by].unique()}\n",
    "\n",
    "for label, group in data.groupby(grouping):\n",
    "    for color_value, group_color in group.groupby(color_by):\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                x=group_color[grouping],\n",
    "                y=group_color[\n",
    "                    \"MedDischargeASR_form\"\n",
    "                ],  # [group[\"Formation Count\"] == 1]\n",
    "                quartilemethod=\"linear\",\n",
    "                name=color_value,\n",
    "                text=group_color[\"US_id\"],\n",
    "                showlegend=not legend_added[color_value],\n",
    "                fillcolor=color[color_value],\n",
    "                line=dict(color=\"black\"),\n",
    "            ),\n",
    "            1,\n",
    "            1,\n",
    "        )\n",
    "        legend_added[color_value] = True\n",
    "\n",
    "\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Formation Discharge ASR (Ohm cm<sup>2</sup>)\",\n",
    "    range=[20, 30],\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "for label, group in data.groupby(grouping):\n",
    "    for color_value, group_color in group.groupby(color_by):\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                x=group_color[grouping],\n",
    "                y=group_color[\"MedDischargeASR_1C\"],  # [group[\"Final 1C Count\"] == 1]\n",
    "                quartilemethod=\"linear\",\n",
    "                name=color_value,\n",
    "                text=group_color[\"US_id\"],\n",
    "                showlegend=not legend_added[color_value],\n",
    "                fillcolor=color[color_value],\n",
    "                line=dict(color=\"black\"),\n",
    "            ),\n",
    "            1,\n",
    "            2,\n",
    "        )\n",
    "        legend_added[color_value] = True\n",
    "\n",
    "\n",
    "fig.update_yaxes(\n",
    "    title_text=\"1C Discharge ASR (Ohm cm<sup>2</sup>)\",\n",
    "    range=[20, 30],\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "for i in range(2):\n",
    "    fig.update_yaxes(\n",
    "        showline=True,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=1,\n",
    "        mirror=True,\n",
    "        ticks=\"outside\",\n",
    "        row=1,\n",
    "        col=i + 1,\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        showline=True,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=1,\n",
    "        mirror=True,\n",
    "        ticks=\"outside\",\n",
    "        row=1,\n",
    "        col=i + 1,\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    # xaxis_title=grouping,\n",
    "    font=dict(\n",
    "        size=16,\n",
    "    ),\n",
    "    # show legend\n",
    "    showlegend=True,\n",
    ")\n",
    "\n",
    "fig.update_traces(boxpoints=\"all\", jitter=0.1)\n",
    "\n",
    "fig.update_xaxes(\n",
    "    categoryorder=\"array\",\n",
    "    categoryarray=data.sort_values([\"batch\"])[grouping].unique(),\n",
    ")  #\n",
    "\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Create dataframe that will feed into the Cell Tiering script\n",
    "######## Tabulate Yielded Cells\n",
    "#Filter yielded Cells\n",
    "data = df_testdata[df_testdata['Yield Count'] == 1]\n",
    "data = data.rename(columns={\"US_id\": \"Cell ID\"})\n",
    "data = data.rename(columns={\"process\": \"Batch\"})\n",
    "\n",
    "#Generate new dataframe to input into Cell Tiering Function\n",
    "YieldedCells = data[data['Yield Count'] == 1]\n",
    "YieldedCells = YieldedCells[[\"Cell ID\",\"Batch\"]]\n",
    "\n",
    "# List of columns to add\n",
    "new_columns = [\n",
    "    \"Cell Status\", \"Pairing Group\", \"Edge Wetting\", \"Thickness\", \"Alignment\", \n",
    "    \"Anode\", \"DischargeASR_1C\", \"Capacity_Co3\", \n",
    "    \"dVdt_fastcharge\", \"total_rank_score\", \"Tray\", \"TrayRow\", \"TrayColumn\"\n",
    "]\n",
    "# Add each column with NaN values\n",
    "for col in new_columns:\n",
    "    YieldedCells[col] = np.nan\n",
    "\n",
    "#Update if yielded cell is on 2L Reliability\n",
    "data = data[[\"Cell ID\",\"Reliability Test Count\"]]\n",
    "# Merge the two dataframes on the 'Cell ID' column\n",
    "merged_df = pd.merge(YieldedCells, data, on='Cell ID', how='right')\n",
    "# Update 'Cell Status' where 'Reliability Test Count' is 1 in merged dataframe\n",
    "merged_df.loc[merged_df['Reliability Test Count'] == 1, 'Cell Status'] = '2L Reliability'\n",
    "# Drop the added columns to keep only the original columns from YieldedCells\n",
    "YieldedCells = merged_df.drop(columns=['Reliability Test Count'])\n",
    "\n",
    "#Update if yielded cell is on already in a multilayer pouch\n",
    "# Query dataframe from database\n",
    "gen = genealogy_v2.get_genealogy_2L('APD|ML|UC|QSC', conn) # \n",
    "# Map the 'US_id' to '6L_cell_id' using '2L_cell_id' in the 'gen' dataframe\n",
    "us_to_ps_mapping = dict(zip(gen['2L_cell_id'], gen['6L_cell_id']))\n",
    "YieldedCells['Cell Status'] = YieldedCells['Cell ID'].map(us_to_ps_mapping)\n",
    "YieldedCells['Cell Status'] = YieldedCells['Cell Status'].fillna('Waiting')\n",
    "\n",
    "### YieldedCells has a empty table for tiering metrics, stitch new code HERE!!!!!!!!!!!!\n",
    "YieldedCells.to_clipboard(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aquiring Thickness Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aquiring Edge Wetting Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aquiring Anode Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aquiring Cathode Misalignment Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ranked cells to 2024-12-02_UCB004A_Tier.xlsx\n"
     ]
    }
   ],
   "source": [
    "##Query Metrics for Tiering\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from qsdc.client import Client\n",
    "import met_client as app\n",
    "import os\n",
    "import unit_cell_metro_metrics_ZI\n",
    "import unit_cell_electrical_yield_and_metrics_v2 as uceym\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import rel_sim as rs\n",
    "import query_tray_samples_V4 as query_tray_samples\n",
    "from importlib import reload\n",
    "from met_client import SearchQuery, ImageAgent\n",
    "from met_client.constants import AnalysisType, ImageSize\n",
    "from image_client.client import ImageClient\n",
    "from image_client.manual_review import convert_manual_reviews_to_dataframe\n",
    "\n",
    "\n",
    "df_pairing = YieldedCells\n",
    "\n",
    "###Pull metrics for each yielded cell\n",
    "\n",
    "## Query HIFI CELL THICKNESS METRICS ##\n",
    "print(\"Aquiring Thickness Data\")\n",
    "agent = app.ImageAgent()        \n",
    "# get US thickness data\n",
    "df_us_thickness = unit_cell_metro_metrics_ZI.get_thickness_metrics(\n",
    "        df_pairing[\"Cell ID\"].str.slice(stop=13).unique(), agent\n",
    "    )\n",
    "# append \"_US\" to column names - \n",
    "df_us_thickness.columns = [f\"{col}_US\" for col in df_us_thickness.columns]\n",
    "df_us_thickness = df_us_thickness.rename(columns={\"sample_US\": \"Cell ID\"})\n",
    "# select desired columns for analysis - updated to use median metrics on the 11-08-24 \n",
    "#selected_columns_df_us_thickness = df_us_thickness[['Cell ID','10mm_eroded_rect_inside_mean_US','0.5mm_eroded_rect_east_mean_US','0.5mm_eroded_rect_west_mean_US','0.5mm_eroded_rect_north_mean_US','0.5mm_eroded_rect_south_mean_US','center_normalized_0.5mm_eroded_rect_outside_mean_US']]\n",
    "selected_columns_df_us_thickness = df_us_thickness[['Cell ID','10mm_eroded_rect_inside_median_US','0.5mm_eroded_rect_east_median_US','0.5mm_eroded_rect_west_median_US','0.5mm_eroded_rect_north_median_US','0.5mm_eroded_rect_south_median_US','center_normalized_0.5mm_eroded_rect_outside_median_US']]\n",
    "# MERGE US DATA\n",
    "df_processing = df_pairing.merge(selected_columns_df_us_thickness, how=\"left\", on=\"Cell ID\")\n",
    "# Define the conditions\n",
    "conditions = [\n",
    "        df_processing['center_normalized_0.5mm_eroded_rect_outside_median_US'] < 1.1\n",
    "        ]\n",
    "    # Define the corresponding values\n",
    "values = [1]\n",
    "# Assign values to the 'Thickness' column\n",
    "df_pairing['Thickness'] = np.where(\n",
    "    df_processing['center_normalized_0.5mm_eroded_rect_outside_median_US'].notna() & conditions[0], values[0], 3\n",
    "    )\n",
    "df_pairing = df_pairing.merge(selected_columns_df_us_thickness[['Cell ID', 'center_normalized_0.5mm_eroded_rect_outside_median_US']], how=\"left\", on=\"Cell ID\")\n",
    "#print(selected_columns_df_us_thickness)\n",
    "\n",
    "## Query Edge Wetting Metrics ##\n",
    "print(\"Aquiring Edge Wetting Data\")\n",
    "df_edge_wetting_metrics = unit_cell_metro_metrics_ZI.get_edge_wetting_metrics(df_pairing['Cell ID'].str.slice(stop=13).unique(), agent)\n",
    "df_pairing = df_pairing.merge(df_edge_wetting_metrics[['sample','median_contour_catholyte_pct']], how=\"left\", left_on=\"Cell ID\", right_on = 'sample').drop(columns = 'sample')\n",
    "#df_pairing['Edge Wetting'] = np.where(df_pairing['median_contour_catholyte_pct'] < 80, 3, 1)\n",
    "\n",
    "\n",
    "## Query Anode tier for pairing \n",
    "print(\"Aquiring Anode Data\")\n",
    "df_anode_metrics = unit_cell_metro_metrics_ZI.get_anode_tier_A1(df_pairing['Cell ID'].str.slice(stop=13).unique(), agent)\n",
    "df_pairing['Anode'] = df_pairing['Cell ID'].str.slice(start=0, stop=16).map(df_anode_metrics.set_index('sample')['A1_anode_tier'])\n",
    "\n",
    "\n",
    "## Query Cathode Misalignment Automated Review ##\n",
    "with ImageClient(host=\"image-api.qscape.app\") as image_client:\n",
    "#image_client = ImageClient()  # Use ImageClient as a context manager\n",
    "    print(\"Aquiring Cathode Misalignment Data\")\n",
    "    for index, row in df_pairing.iterrows():\n",
    "        sample_name = row['Cell ID']\n",
    "        image_search = SearchQuery(\n",
    "            sample_prefix=sample_name,    \n",
    "            a_type=AnalysisType.CONTRAST,      \n",
    "            lregex=\"nordson_matrix-us-stitched-corners$\",   \n",
    "            )\n",
    "        image_agent = ImageAgent()\n",
    "        image_results = image_agent.search(query=image_search)\n",
    "        # Check if 'cathode_alignment_custom_model_prediction' exists in the image results\n",
    "        if 'cathode_alignment_custom_model_prediction' in image_results:\n",
    "            cathodemisalignment = image_results['cathode_alignment_custom_model_prediction']\n",
    "            if cathodemisalignment.iloc[0] == \"go\":\n",
    "                df_pairing.at[index, 'Alignment'] = 1\n",
    "                #print(f\"{sample_name} is a 1 based on CV model\")\n",
    "            elif cathodemisalignment.iloc[0] == \"no-go\":\n",
    "                df_pairing.at[index, 'Alignment'] = 3\n",
    "                #print(f\"{sample_name} is a 3 based on CV model\")\n",
    "\n",
    "        #Check manual review of cathode misalignment and edge wetting\n",
    "        manual_reviews = image_client.get_manual_reviews(samples=[sample_name], include_history=True)\n",
    "        manualreviewCM = convert_manual_reviews_to_dataframe(manual_reviews, include_modified_date=True)\n",
    "        if 'cathode_alignment' in manualreviewCM:\n",
    "            if not manualreviewCM.empty and manualreviewCM['cathode_alignment'].notnull().any():\n",
    "                df_pairing.at[index, 'Alignment'] = manualreviewCM['cathode_alignment'].iloc[0]\n",
    "                #print(f\"Manual Review corrected {sample_name} cathode misalignment to {manualreviewCM['cathode_alignment'].iloc[0]}\")\n",
    "        if 'edge_wetting' in manualreviewCM:\n",
    "            if not manualreviewCM.empty and manualreviewCM['edge_wetting'].notnull().any():\n",
    "                df_pairing.at[index, 'median_contour_catholyte_pct'] = manualreviewCM['edge_wetting'].iloc[0]\n",
    "                #print(f\"Manual Review corrected {sample_name} edge wetting to {manualreviewCM['edge_wetting'].iloc[0]}\")\n",
    "\n",
    "conditions = [\n",
    "    df_pairing['median_contour_catholyte_pct'] < 80,\n",
    "    (df_pairing['median_contour_catholyte_pct'] >= 80) & (df_pairing['median_contour_catholyte_pct'] <= 98),\n",
    "    df_pairing['median_contour_catholyte_pct'] > 98\n",
    "]\n",
    "choices = [3, 2, 1]\n",
    "df_pairing['Edge Wetting'] = np.select(conditions, choices)\n",
    "\n",
    "# Remove duplicate rows across all columns\n",
    "#df_pairing = df_pairing.drop_duplicates()\n",
    "\n",
    "\n",
    "# Select columns that we are considering for tiering\n",
    "columns_to_consider = [\n",
    "    'Alignment',\n",
    "    'Anode',\n",
    "    'Thickness', \n",
    "    'Edge Wetting'\n",
    "]\n",
    "# Calculate the minimum value for each row across the selected columns\n",
    "#df_pairing['Total'] = df_pairing[columns_to_consider].max(axis=1)\n",
    "# Ensure these columns exist in df_pairing before calculating 'Total'\n",
    "missing_columns = [col for col in columns_to_consider if col not in df_pairing.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Warning: The following columns are missing from df_pairing: {missing_columns}\")\n",
    "\n",
    "# Calculate the maximum value for each row across the specified columns\n",
    "# Use only the columns that exist in df_pairing\n",
    "valid_columns = [col for col in columns_to_consider if col in df_pairing.columns]\n",
    "df_pairing['Pairing Group'] = df_pairing[valid_columns].max(axis=1)\n",
    "\n",
    "## Query electrical metrics for pairing ##\n",
    "df_electrical_yield_metrics = uceym.get_electrical_yield_and_metrics(df_pairing[\"Cell ID\"])\n",
    "df_pairing = pd.merge(df_pairing, df_electrical_yield_metrics, left_on='Cell ID', right_on='US_id', how='left')\n",
    "# df_pairing['Capacity_Difference'] = abs(df_pairing['AMSDischargeCapacity_Co3'] - (-0.4805 * df_pairing['MedDischargeASR_1C'] + 207.0932))\n",
    "\n",
    "## Query tray location ##\n",
    "sample_names_group = df_pairing[\"Cell ID\"]\n",
    "df_tray = query_tray_samples.get_sample_tray(sample_names_group)\n",
    "df_pairing = pd.merge(df_pairing, df_tray[['sample_name', 'tray_id', 'row_index', 'col_index', 'modified']], left_on='Cell ID', right_on='sample_name', how='left')\n",
    "\n",
    "\n",
    "# Select and reorder the specified columns\n",
    "final_columns = [\n",
    "            \"Cell ID\", \"Batch\", \"Cell Status\", \"Pairing Group\", \n",
    "            \"Edge Wetting\",\"median_contour_catholyte_pct\", \"Thickness\", \"center_normalized_0.5mm_eroded_rect_outside_median_US\", \"Alignment\", \n",
    "            \"Anode\", \"MedDischargeASR_1C\", \n",
    "            \"DischargeCapacity_Co3\", \"dVdt_delta_fastcharge\", \n",
    "            \"total_rank_score\", \"tray_id\", \"row_index\", \"col_index\"\n",
    "        ]\n",
    "\n",
    "# Filter the DataFrame to only include the specified columns\n",
    "rank = df_pairing[final_columns]\n",
    "\n",
    "# Save the ranked cells to an Excel file\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_{batches}_Tier.xlsx'\n",
    "rank.to_excel(output_name, index=False)\n",
    "print(f\"Saved ranked cells to {output_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format and prepare dataframes for TOPSIS Rank and Grouping\n",
    "\n",
    "#Rename \n",
    "TieredCells = rank\n",
    "\n",
    "#Keep Tier 1 Cells that have not been grouped\n",
    "TieredCells = TieredCells[TieredCells[\"Cell Status\"] == \"Waiting\"]\n",
    "\n",
    "#Keep Tier 1 Cells to commence pairing of Tier 1 groups\n",
    "TieredOneCells = TieredCells[TieredCells['Pairing Group'] == 1].copy()\n",
    "TieredTwoThreeCells = TieredCells[TieredCells['Pairing Group'] != 1].copy()\n",
    "\n",
    "# Remove duplicate rows across all columns\n",
    "TieredOneCells = TieredOneCells.drop_duplicates()\n",
    "TieredTwoThreeCells = TieredTwoThreeCells.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate Cell Score and Ranking for Tier 1 Cells using TOPSIS method\n",
    "# Define the criteria and their corresponding optimization directions\n",
    "criteria = ['median_contour_catholyte_pct', 'center_normalized_0.5mm_eroded_rect_outside_median_US', 'MedDischargeASR_1C', 'DischargeCapacity_Co3'] \n",
    "weights = [0.15, 0.25, 0.35, 0.25]  # Equal weights (adjust if needed)\n",
    "optimization_directions = ['High', 'Proximity', 'Low', 'High']  # 'Proximity' for 'Thickness'\n",
    "target_criteria = ['center_normalized_0.5mm_eroded_rect_outside_median_US']  # Criteria optimized for proximity\n",
    "targets = [1.0]  # Target value for 'Thickness'\n",
    "\n",
    "\n",
    "def topsis_with_target_Tier1(df, criteria, weights, optimization_directions, target_criteria=None, targets=None):\n",
    "    \n",
    "    # Ensure numeric data for criteria\n",
    "    df[criteria] = df[criteria].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    normalized_matrix = df[criteria].copy()\n",
    "    \n",
    "    # Step 1: Normalize the Decision Matrix\n",
    "    for col in criteria:\n",
    "        if target_criteria and col in target_criteria:\n",
    "            # Calculate absolute deviation from target\n",
    "            target = targets[target_criteria.index(col)]\n",
    "            normalized_matrix[col] = np.abs(df[col] - target)  # Deviation\n",
    "        else:\n",
    "            # Standard normalization\n",
    "            normalized_matrix[col] = normalized_matrix[col] / np.sqrt((normalized_matrix[col]**2).sum())\n",
    "    \n",
    "    # Step 2: Weight the Criteria\n",
    "    weighted_matrix = normalized_matrix * weights\n",
    "    \n",
    "    # Step 3: Identify Ideal and Negative Ideal Solutions\n",
    "    ideal_solution = []\n",
    "    negative_ideal_solution = []\n",
    "    for col, direction in zip(criteria, optimization_directions):\n",
    "        if target_criteria and col in target_criteria:\n",
    "            # For deviation-based criteria, minimize the deviation\n",
    "            ideal_solution.append(weighted_matrix[col].min())\n",
    "            negative_ideal_solution.append(weighted_matrix[col].max())\n",
    "        else:\n",
    "            if direction == 'High':\n",
    "                ideal_solution.append(weighted_matrix[col].max())\n",
    "                negative_ideal_solution.append(weighted_matrix[col].min())\n",
    "            elif direction == 'Low':\n",
    "                ideal_solution.append(weighted_matrix[col].min())\n",
    "                negative_ideal_solution.append(weighted_matrix[col].max())\n",
    "    \n",
    "    ideal_solution = np.array(ideal_solution)\n",
    "    ideal_solution = np.array([\n",
    "        weighted_matrix.iloc[:, idx].max() if direction == \"maximize\" else weighted_matrix.iloc[:, idx].min()\n",
    "        for idx, direction in enumerate(optimization_directions)\n",
    "    ])\n",
    "\n",
    "    negative_ideal_solution = np.array(negative_ideal_solution)\n",
    "    negative_ideal_solution = np.array([\n",
    "        weighted_matrix.iloc[:, idx].min() if direction == \"maximize\" else weighted_matrix.iloc[:, idx].max()\n",
    "        for idx, direction in enumerate(optimization_directions)\n",
    "    ])\n",
    "    \n",
    "    # Step 4: Calculate Distances\n",
    "    distances_to_ideal = np.sqrt(((weighted_matrix - ideal_solution)**2).sum(axis=1))\n",
    "    distances_to_negative = np.sqrt(((weighted_matrix - negative_ideal_solution)**2).sum(axis=1))\n",
    "    \n",
    "    # Step 5: Calculate TOPSIS Score\n",
    "    topsis_score = distances_to_negative / (distances_to_ideal + distances_to_negative)\n",
    "    return topsis_score\n",
    "\n",
    "# Calculate TOPSIS scores\n",
    "TieredOneCells['total_rank_score'] = topsis_with_target_Tier1(TieredOneCells, criteria, weights, optimization_directions, target_criteria, targets)\n",
    "# Rank alternatives by TOPSIS score (higher score is better)\n",
    "TieredOneCells['Rank'] = TieredOneCells['total_rank_score'].rank(ascending=False)\n",
    "# Arrange Cells by best ranking at the top\n",
    "TieredOneCells.sort_values(by='Rank', ascending=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group starting at rank [1, 2, 3]: ['UCB004AA-US00-08', 'UCB004AA-US00-04', 'UCB004AA-US00-22']\n",
      "ASR range: 1.157399999999999, ASR mean-min: 0.4656333333333329\n",
      "Established group starting at rank [1, 2, 3]: ['UCB004AA-US00-08', 'UCB004AA-US00-04', 'UCB004AA-US00-22']\n",
      "Rows in remainder after processing: 0\n",
      "Final DataFrame has 3 rows.\n"
     ]
    }
   ],
   "source": [
    "df = TieredOneCells\n",
    "df = df.reset_index(drop=True)\n",
    "df['Rank'] = (df.index + 1).astype(int)  # Add 1-based rank as integers\n",
    "remainder = df.copy()  # Start with a copy of the input DataFrame\n",
    "retries = 0  # Track retries for reprocessing the remainder\n",
    "\n",
    "n = 3\n",
    "\n",
    "#Initialize variable and dataframe \n",
    "groupnumber = 1  # Initialize group number\n",
    "df_final = pd.DataFrame()  # Initialize an empty DataFrame for the final result\n",
    "grouped_list = [] # List to collect all the groups for final concatenation\n",
    "cellstart = 0\n",
    "\n",
    "# Keep track of processed indices\n",
    "processed_indices = set()\n",
    "\n",
    "while len(remainder) >= n + cellstart:\n",
    "    # Find the next candidate group by slicing remainder\n",
    "    candidate_group = remainder.iloc[cellstart:cellstart+n].copy()\n",
    "    print(f\"Processing group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "\n",
    "    # Calculate criteria for the group\n",
    "    asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "    asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "\n",
    "    print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "\n",
    "    # Check if group satisfies criteria and replace cells if necessary\n",
    "    if asr_range >= 2.0:\n",
    "        # Replace the highest ASR cell\n",
    "        max_idx = candidate_group['MedDischargeASR_1C'].idxmax()\n",
    "        problem_cell_id = candidate_group.loc[max_idx, 'Cell ID']\n",
    "        print(f\"ASR range is too high at {asr_range}, replacing problem cell: {problem_cell_id}\")\n",
    "        # Replace the highest ASR cell here (you can replace it based on your logic)\n",
    "        i_range = 0\n",
    "\n",
    "        while asr_range >=2.0:\n",
    "            if n + i_range >= len(remainder):  # Check if the index is valid\n",
    "                print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                cellstart = cellstart +1\n",
    "                break\n",
    "            candidate_group.loc[max_idx] = remainder.iloc[n + i_range] # Replace the highest ASR cell\n",
    "            replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Cell ID of the replacement cell\n",
    "            print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "            # Check for duplicates in 'Cell ID' within the candidate group\n",
    "            duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "            while duplicate_count > 0:\n",
    "                print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_range.\")\n",
    "                i_range += 1  # Increment if there are duplicates\n",
    "                if n + i_range < len(remainder):\n",
    "                    candidate_group.loc[max_idx] = remainder.iloc[n + i_range]\n",
    "                    replacement_cell_id = remainder.iloc[n + i_range]['Cell ID']  # Replacement cell ID\n",
    "                    print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                    duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "\n",
    "            asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "            i_range = i_range+1\n",
    "\n",
    "    if asr_mean_min >= 0.5:\n",
    "        # Replace the lowest ASR cell\n",
    "        min_idx = candidate_group['MedDischargeASR_1C'].idxmin()\n",
    "        problem_cell_id = candidate_group.loc[min_idx, 'Cell ID']\n",
    "        print(f\"Minimum ASR is too low at difference of {asr_mean_min}, replacing problem cell: {problem_cell_id}\")\n",
    "        # Replace the lowest ASR cell here (you can replace it based on your logic)\n",
    "        i_mean_min = 0\n",
    "\n",
    "        while asr_mean_min >= 0.5:\n",
    "            if n + i_mean_min >= len(remainder):  # Check if the index is valid\n",
    "                print(f\"No more rows in remainder to replace {problem_cell_id}.\")\n",
    "                cellstart = cellstart +1\n",
    "                break\n",
    "            candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "            replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "            print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "\n",
    "            # Check for duplicates in 'Cell ID' within the candidate group\n",
    "            duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "            while duplicate_count > 0:\n",
    "                print(f\"Found {duplicate_count} duplicate(s) of 'Cell ID' in candidate_group. Incrementing i_mean_min.\")\n",
    "                i_mean_min += 1  # Increment if there are duplicates\n",
    "                if n + i_mean_min < len(remainder):\n",
    "                    candidate_group.loc[min_idx] = remainder.iloc[n + i_mean_min]\n",
    "                    replacement_cell_id = remainder.iloc[n + i_mean_min]['Cell ID']  # Replacement cell ID\n",
    "                    print(f\"Replacing {problem_cell_id} with {replacement_cell_id}\")\n",
    "                    duplicate_count = candidate_group['Cell ID'].duplicated(keep=False).sum()\n",
    "            \n",
    "            asr_range = candidate_group['MedDischargeASR_1C'].max() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            asr_mean_min = candidate_group['MedDischargeASR_1C'].mean() - candidate_group['MedDischargeASR_1C'].min()\n",
    "            print(f\"ASR range: {asr_range}, ASR mean-min: {asr_mean_min}\")\n",
    "            i_mean_min = i_mean_min+1\n",
    "\n",
    "\n",
    "    if asr_range < 2.0 and asr_mean_min < 0.5:\n",
    "        print(f\"Established group starting at rank {candidate_group['Rank'].tolist()}: {candidate_group['Cell ID'].tolist()}\")\n",
    "        # Assign the group number\n",
    "        candidate_group['Group Number'] = groupnumber\n",
    "        # Add the rows to the final list\n",
    "        grouped_list.append(candidate_group)\n",
    "        # Drop rows in \"remainder\" where \"Cell ID\" matches any in \"candidate_group\"\n",
    "        remainder = remainder[~remainder['Cell ID'].isin(candidate_group['Cell ID'])]\n",
    "        # Update processed indices\n",
    "        processed_indices.update(candidate_group.index)\n",
    "        # Update group number for the next group\n",
    "        groupnumber += 1\n",
    "        # Print status\n",
    "        print(f\"Rows in remainder after processing: {len(remainder)}\")\n",
    "\n",
    "\n",
    "# Check if grouped_list contains valid DataFrames\n",
    "if not grouped_list:\n",
    "    print(\"No valid groups were formed. Creating an empty DataFrame.\")\n",
    "    df_final = pd.DataFrame()  # Handle case where no groups are created\n",
    "else:\n",
    "    df_final = pd.concat(grouped_list, ignore_index=True)\n",
    "# Add 'Remainder' label to remaining rows, if applicable\n",
    "if not remainder.empty:\n",
    "    remainder['Group Number'] = 'Remainder'\n",
    "    df_final = pd.concat([df_final, remainder], ignore_index=True)\n",
    "\n",
    "# Add 'Remainder' label to remaining rows\n",
    "#remainder['Group Number'] = 'Remainder'\n",
    "\n",
    "# Add the remainder rows to df_final\n",
    "#df_final = pd.concat([df_final, remainder], ignore_index=True)\n",
    "\n",
    "# Format final DataFrame\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "df_final = df_final.drop(columns=['median_contour_catholyte_pct', 'center_normalized_0.5mm_eroded_rect_outside_median_US','Rank']) #drop rows that we don't need to show in the spreadsheet\n",
    "df_final = df_final[['Group Number'] + [col for col in df_final.columns if col != 'Group Number']] #move 'Group Number' to first column\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(f\"Final DataFrame has {df_final.shape[0]} rows.\")\n",
    "\n",
    "# Save the ranked cells to an Excel file\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_{batches}_Tier_1_Grouped.xlsx'\n",
    "df_final.to_excel(output_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Function to rearrange rows in pyramid order within each group\n",
    "def pyramid_order(group):\n",
    "    # Sort values within the group\n",
    "    sorted_group = group.sort_values(by='MedDischargeASR_1C', ascending=True)\n",
    "    # Rearrange in pyramid order\n",
    "    pyramid = []\n",
    "    while len(sorted_group) > 0:\n",
    "        if len(pyramid) % 2 == 0:  # Add largest remaining value to the middle\n",
    "            pyramid.append(sorted_group.iloc[-1])\n",
    "            sorted_group = sorted_group[:-1]\n",
    "        else:  # Add smallest remaining value to the sides\n",
    "            pyramid.insert(0, sorted_group.iloc[0])\n",
    "            sorted_group = sorted_group[1:]\n",
    "    # Reconstruct DataFrame\n",
    "    return pd.DataFrame(pyramid)\n",
    "\n",
    "# Apply the function to each group\n",
    "df_pyramid = df_final.groupby('Group Number', group_keys=False).apply(pyramid_order)\n",
    "\n",
    "# Reset the index if needed\n",
    "df_pyramid = df_pyramid.reset_index(drop=True)\n",
    "\n",
    "# Save the ranked cells to an Excel file\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_{batches}_Tier_1_Grouped_Pyramid_Order.xlsx'\n",
    "df_pyramid.to_excel(output_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
