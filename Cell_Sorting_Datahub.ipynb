{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tray name(s) for filtering\n",
    "\n",
    "batches = \"UCB007|UCB008|UCB009|UCB010\" #specify which batches you want to filter, leave blank if you want to see all batches (\" \")\n",
    "\n",
    "tray_name = [\"0095584-1.6\", \"0086469-1.0-1\"]   # prefix for metrology trays and thermoform trays\n",
    "\n",
    "LastFewDays = 2 #the amount of days we want to check for scan history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from qsdc.client import Client\n",
    "import met_client as app\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Alignment\n",
    "\n",
    "\n",
    "# Create the QuantumScape data client\n",
    "qs_client = Client()\n",
    "conn = qs_client.get_mysql_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Cells in Recent Trays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tray_name list into a regular expression pattern\n",
    "regexp_pattern = \"|\".join(tray_name)\n",
    "\n",
    "# Query to get relevant data from the database\n",
    "query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM production_2011_beta_1.tray_links\n",
    "    WHERE barcode_data REGEXP '{regexp_pattern}'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "data = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Filter the data by 'idtray' to keep only rows with 'tray_unit_cell_automation_4x3_001'\n",
    "data = data[data['idtray'] == 'tray_unit_cell_automation_4x3_001']\n",
    "\n",
    "\n",
    "# Convert the list 'batches' into a regex pattern\n",
    "#batches_pattern = \"|\".join(map(re.escape, batches))  # Ensures special characters are escaped\n",
    "# Apply the regex pattern in str.contains()\n",
    "data = data[data['sample_name'].str.contains(batches, regex=True, na=False)]\n",
    "\n",
    "\n",
    "# Keep only specific columns\n",
    "keep_columns = ['barcode_data', 'sample_name', 'row_index', 'col_index', 'modified']\n",
    "newdata_group_1 = data.loc[:, keep_columns]\n",
    "\n",
    "# Rename 'barcode_data' to 'tray_id'\n",
    "newdata_group_1 = newdata_group_1.rename(columns={'barcode_data': 'tray_id', 'modified':'Scan Time'})\n",
    "\n",
    "# Convert 'col_index' to integers to remove the decimal point\n",
    "newdata_group_1['col_index'] = newdata_group_1['col_index'].astype(int)\n",
    "\n",
    "# Reorder columns to move 'sample_name' to the first position\n",
    "newdata_group_1 = newdata_group_1[['sample_name', 'tray_id', 'row_index', 'col_index', 'Scan Time']]\n",
    "newdata_group_1 = newdata_group_1.loc[newdata_group_1.groupby('sample_name')['Scan Time'].idxmax()]\n",
    "\n",
    "# Sort the data with 'modified' in descending order and other columns in ascending order\n",
    "TrayHistory = newdata_group_1.sort_values(by=['Scan Time', 'tray_id', 'row_index', 'col_index'], \n",
    "                                             ascending=[False, True, True, True])\n",
    "\n",
    "\n",
    "# Convert 'Scan Time' column to datetime format if not already\n",
    "TrayHistory['Scan Time'] = pd.to_datetime(TrayHistory['Scan Time'])\n",
    "\n",
    "# Compute the cutoff date\n",
    "cutoff_date = datetime.now() - timedelta(days=LastFewDays)\n",
    "\n",
    "# Filter rows where 'Scan Time' is within the last 'x' days abd 'sample_name' contains \"US\"\n",
    "TrayHistory= TrayHistory[\n",
    "    (TrayHistory['Scan Time'] >= cutoff_date) & \n",
    "    (TrayHistory['sample_name'].str.contains(\"US\", na=False))\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Cell Data from Datahub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EDM02\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\qsdc\\datahub\\datahub.py:88: DtypeWarning: Columns (5,15,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(bytesIO(blob_bytes))\n"
     ]
    }
   ],
   "source": [
    "## Query Data from Datahub\n",
    "\n",
    "#Pull cell metrology data from datahub, both standard/auto metrology and manual review\n",
    "dfctq = qs_client.data_hub.get_dataset(dataset = 'MFG-60L-UC-CTQ') ## standard metro review of unit cells\n",
    "dfmr = qs_client.data_hub.get_dataset(dataset = 'MFG-60L-UC-MR') ## manual review of unit cells\n",
    "\n",
    "#Pull geneology/multilayer info\n",
    "dmlg = qs_client.data_hub.get_dataset(dataset = 'MFG-80L-ML-PRODUCTION') ##multilayer info (ML_id)\n",
    "dmlg = dmlg.dropna(subset=\"US_id\").drop_duplicates()\n",
    "#dgen = qs_client.data_hub.get_dataset(dataset = 'MFG-MASTER-GENEALOGY') ##multilayer info (ML_id)\n",
    "danc = qs_client.data_hub.get_dataset(\"MFG-UNIT-CELL-ANCESTRY\")\n",
    "\n",
    "#Pull Cathode Mass data\n",
    "cathode_mass = qs_client.data_hub.get_dataset(\"MFG-50L-CATHODE-CTQ\")\n",
    "cathode_mass = cathode_mass[[\"PU_id\", \"cathode_dry_mass\"]]\n",
    "danc = danc[[\"US_id\",\"PU_id\"]].dropna(subset=\"US_id\").drop_duplicates()\n",
    "danc = danc.merge(cathode_mass, on = \"PU_id\", how = \"left\").sort_values([\"US_id\"])\n",
    "danc = danc.rename(columns={'US_id': 'US_ID'}) \n",
    "\n",
    "\n",
    "#Pull cell electrical test data\n",
    "dfc = qs_client.data_hub.get_dataset(dataset = 'MFG-60L-UNIT-CELL-TEST-CYCLE') ##electrical test data of unit cells\n",
    "cols = [\n",
    "    \"US_id\",\n",
    "    \"TestCycleStart_datetime_first\",\n",
    "    \"TestCycleStart_datetime\",\n",
    "    \"UCT_Version\",\n",
    "    \"idtest_recipe\",\n",
    "    \"RunIndex\",\n",
    "    \"CycleIndex\",\n",
    "    \"dvdt\",\n",
    "    \"recipe_dvdt_range\",\n",
    "    \"CeilingHoldTime\",\n",
    "    \"CE\",\n",
    "    \"CapacityChargeFraction\",\n",
    "    \"CeilingRestVoltage\",\n",
    "    \"AMSDcCapacity\",\n",
    "    \"CycleFailure\",\n",
    "    \"AnyFailure\",\n",
    "    \"MedDcASR\",\n",
    "    \"DischargeCapacity\"\n",
    "]\n",
    "\n",
    "#Filter datahub rows by batches \n",
    "    # Keep cells from batches of interest and remove those that failed screening\n",
    "\n",
    "yielded_dfctq = dfctq[dfctq['US_id'].isin(TrayHistory['sample_name'])]\n",
    "yielded_dfmr = dfmr[dfmr['US_id'].isin(TrayHistory['sample_name'])]\n",
    "batches = \"CustomList\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Automated and Manual Review Tiering Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overwrite automated results with manual review results\n",
    "# First, merge the DataFrames on 'US_id' to align rows\n",
    "merged_df = yielded_dfctq.merge(yielded_dfmr[['US_id', 'edge_thickness_tier_us_mr', 'A1_anode_tier_top_us_mr', 'A1_anode_tier_bottom_us_mr',\n",
    "                                              'cathode_alignment_custom_model_tier_us_mr', 'median_contour_catholyte_pct_us_mr', 'max_f2f_distance_us','disposition_mr', 'failure_modes_mr']], on='US_id', how='left')\n",
    "# Then, overwrite 'edge_thickness_tier_us' in 'filtered_dfctq' where 'edge_thickness_tier_us_mr' has a value\n",
    "merged_df['edge_thickness_tier_us'] = merged_df['edge_thickness_tier_us_mr'].combine_first(merged_df['edge_thickness_tier_us'])\n",
    "# Then, overwrite 'A1_anode_tier_top_us' in 'filtered_dfctq' where 'A1_anode_tier_top_us_mr' has a value\n",
    "merged_df['A1_anode_tier_top_us'] = merged_df['A1_anode_tier_top_us_mr'].combine_first(merged_df['A1_anode_tier_top_us'])\n",
    "# Then, overwrite 'A1_anode_tier_bottom_us' in 'filtered_dfctq' where 'A1_anode_tier_bottom_us_mr' has a value\n",
    "merged_df['A1_anode_tier_bottom_us'] = merged_df['A1_anode_tier_bottom_us_mr'].combine_first(merged_df['A1_anode_tier_bottom_us'])\n",
    "# Then, overwrite 'A1_anode_tier_bottom_us' in 'filtered_dfctq' where 'A1_anode_tier_bottom_us_mr' has a value\n",
    "merged_df['cathode_alignment_custom_model_tier_us'] = merged_df['cathode_alignment_custom_model_tier_us_mr'].combine_first(merged_df['cathode_alignment_custom_model_tier_us'])\n",
    "#merged_df.loc[merged_df['cathode_alignment_custom_model_tier_us_mr'].notna(), 'cathode_alignment_custom_model_tier_us'] = \\\n",
    "    #merged_df['cathode_alignment_custom_model_tier_us_mr']\n",
    "\n",
    "# Then, overwrite 'A1_anode_tier_bottom_us' in 'filtered_dfctq' where 'A1_anode_tier_bottom_us_mr' has a value\n",
    "merged_df['median_contour_catholyte_pct_us'] = merged_df['median_contour_catholyte_pct_us_mr'].combine_first(merged_df['median_contour_catholyte_pct_us'])\n",
    "# Then, overwrite 'disposition_us' in 'disposition_mr' has a value\n",
    "merged_df['disposition'] = merged_df['disposition_mr'].combine_first(merged_df['disposition'])\n",
    "# Then, overwrite 'disposition_us' in 'disposition_mr' has a value\n",
    "merged_df['failure_modes'] = merged_df['failure_modes_mr'].combine_first(merged_df['failure_modes'])\n",
    "# Drop the 'edge_thickness_tier_us_mr' column if you don't need it\n",
    "dfctq_updated = merged_df.drop(columns=['edge_thickness_tier_us_mr', 'A1_anode_tier_top_us_mr', 'A1_anode_tier_bottom_us_mr',\n",
    "                                              'cathode_alignment_custom_model_tier_us_mr', 'median_contour_catholyte_pct_us_mr', 'disposition_mr','failure_modes_mr' ])\n",
    "\n",
    "\n",
    "#Update Final Tier of Cells\n",
    "conditions = [\n",
    "    dfctq_updated['disposition'] == 'Tier 1',\n",
    "    dfctq_updated['disposition'] == 'Tier 2',\n",
    "    dfctq_updated['disposition'] == 'Fail',\n",
    "    dfctq_updated['disposition'] == 'Scrap',\n",
    "    dfctq_updated['disposition'] == 'Missing Data',\n",
    "]\n",
    "choices = ['1', '2', '3','Scrapped', 'TBD']\n",
    "dfctq_updated['Tier'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill in tiering metrics based on data pulled from datahub\n",
    "\n",
    "## Rename Spreadsheet and sample/batch columns\n",
    "CellTiering = dfctq_updated[['US_id']].rename(columns={'US_id': 'Cell ID'}) \n",
    "CellTiering['Batch'] = dfctq_updated['US_process_flow'] #Create Tiering Spreadsheet\n",
    "# Create columns for final spreadsheet\n",
    "new_columns = [\"Cell Status\", \"Cell Tier\", \"Edge Wetting\", \"Thickness\", \"Alignment\", \"Anode\",\"Film-to-Film Distance\", \"Cathode Mass\"]\n",
    "for col in new_columns:\n",
    "    CellTiering[col] = np.nan\n",
    "\n",
    "\n",
    "##Update Cell Tiering metrics in final spreadsheet\n",
    "# Merge the dfctq_updated with the CellTiering\n",
    "merged_df = CellTiering.merge(\n",
    "    dfctq_updated[['US_id', 'Tier', 'median_contour_catholyte_pct_us', 'edge_thickness_tier_us', 'center_normalized_0_5mm_eroded_rect_outside_median_us', \n",
    "                   'cathode_alignment_custom_model_tier_us', 'A1_anode_tier_top_us','A1_anode_tier_bottom_us', 'max_f2f_distance_us']],\n",
    "    left_on='Cell ID',\n",
    "    right_on='US_id',\n",
    "    how='left'\n",
    ")\n",
    "# Update final 'Cell Tier' columns with data from dfctq_updated\n",
    "merged_df['Cell Tier'] = merged_df['Tier']\n",
    "#Update 'Edge Wetting' column\n",
    "EWconditions = [\n",
    "    merged_df['median_contour_catholyte_pct_us'] < 80,\n",
    "    (merged_df['median_contour_catholyte_pct_us'] >= 80) & (merged_df['median_contour_catholyte_pct_us'] <= 98),\n",
    "    merged_df['median_contour_catholyte_pct_us'] > 98\n",
    "]\n",
    "EWchoices = [3, 2, 1]\n",
    "merged_df['Edge Wetting'] = np.select(EWconditions, EWchoices)\n",
    "#Update 'Thickness' Column\n",
    "merged_df['Thickness'] = merged_df['edge_thickness_tier_us']\n",
    "#Update Alignment' Column\n",
    "merged_df['Alignment'] = merged_df['cathode_alignment_custom_model_tier_us']\n",
    "#Update 'Anode' Column\n",
    "merged_df['Anode'] = np.maximum(merged_df['A1_anode_tier_top_us'], merged_df['A1_anode_tier_bottom_us'])\n",
    "#Update 'Film-to-Film Distance Column\n",
    "merged_df.loc[merged_df['max_f2f_distance_us'] > 0.40, 'Film-to-Film Distance'] = 'High' #merged_df['Film-to-Film Distance'] = 'Low'  # Default value\n",
    "#Update the 'Cathode Mass' Column\n",
    "merged_df = merged_df.merge(danc[['US_ID', 'cathode_dry_mass']], left_on='Cell ID', right_on='US_ID', how='left')\n",
    "merged_df['Cathode Mass'] = merged_df['cathode_dry_mass']\n",
    "\n",
    "# Drop the extra columns from dfctq_updated\n",
    "CellTiering = merged_df.drop(columns=['US_id', 'Tier', 'edge_thickness_tier_us','cathode_alignment_custom_model_tier_us',\n",
    "                                      'A1_anode_tier_top_us','A1_anode_tier_bottom_us','max_f2f_distance_us', 'US_ID', 'cathode_dry_mass'])\n",
    "\n",
    "\n",
    "\n",
    "## Asign Final Tier to every cell\n",
    "# Select columns that we are considering for tiering\n",
    "columns_to_consider = ['Alignment', 'Anode', 'Thickness', 'Edge Wetting']\n",
    "CellTiering['Alignment'] = CellTiering['Alignment'].fillna(0)  # Replace NaN with 0 (or another placeholder)\n",
    "CellTiering['Alignment'] = CellTiering['Alignment'].astype(int) #convert to integer\n",
    "CellTiering['Anode'] = CellTiering['Anode'].fillna(0)  # Replace NaN with 0 (or another placeholder)\n",
    "CellTiering['Anode'] = CellTiering['Anode'].astype(int) #convert to intege\n",
    "CellTiering['Thickness'] = CellTiering['Thickness'].fillna(0)  # Replace NaN with 0 (or another placeholder)\n",
    "CellTiering['Thickness'] = CellTiering['Thickness'].astype(int) #convert to integer\n",
    "CellTiering['Edge Wetting'] = CellTiering['Edge Wetting'].fillna(0)\n",
    "CellTiering['Edge Wetting'] = CellTiering['Edge Wetting'].astype(int) #convert to integer\n",
    "#Fill out Cell Tier\n",
    "CellTiering['Cell Tier'] = CellTiering[columns_to_consider].max(axis=1) #Update cell tier based on avaialable tiering metrics\n",
    "missing_or_zero = (CellTiering[columns_to_consider].isnull() | (CellTiering[columns_to_consider] == 0)).any(axis=1) #Identify cells that are missing data\n",
    "CellTiering.loc[missing_or_zero, 'Cell Tier'] = 0 # Update 'Cell Status' for these rows\n",
    "\n",
    "RecentCellTiers = TrayHistory.merge(CellTiering, left_on='sample_name', right_on='Cell ID', how='left')\n",
    "RecentCellTiers['Matrix'] = RecentCellTiers['Alignment'].apply(lambda x: 'Done' if x in [1, 2, 3] else 'Waiting for Matrix')\n",
    "RecentCellTiers['HiFi'] = RecentCellTiers['Anode'].apply(lambda x: 'Done' if x in [1, 2, 3] else 'Waiting for HiFi')\n",
    "RecentCellTiers = RecentCellTiers[['sample_name', 'Batch', 'tray_id', 'row_index', 'col_index', 'Scan Time','Matrix', 'HiFi', 'Cell Tier']] \n",
    "\n",
    "\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "output_name = f'{current_date}_Sorting list from last {LastFewDays} days.xlsx'\n",
    "RecentCellTiers.to_excel(output_name, index=False)\n",
    "RecentCellTiers\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "output_name = f'{current_date}_Sorting list from last {LastFewDays} days.xlsx'\n",
    "RecentCellTiers.to_excel(output_name, index=False)\n",
    "\n",
    "# Load the workbook and worksheet\n",
    "wb = load_workbook(output_name)\n",
    "ws = wb.active\n",
    "\n",
    "# Define color mappings\n",
    "tier_colors = {\n",
    "    0: \"CECECE\",  # Grey\n",
    "    1: \"7DDA58\",  # Light Green\n",
    "    2: \"FFECA1\",  # Yellow\n",
    "    3: \"EFC3CA\"   # Red\n",
    "}\n",
    "\n",
    "# Find the column index of \"Cell Tier\"\n",
    "tier_column_index = None\n",
    "for col_num, col_cells in enumerate(ws.iter_cols(min_row=1, max_row=1), start=1):\n",
    "    if col_cells[0].value == \"Cell Tier\":\n",
    "        tier_column_index = col_num\n",
    "        break\n",
    "\n",
    "# Apply color formatting to the entire row & center-align text\n",
    "if tier_column_index:\n",
    "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):\n",
    "        cell = row[tier_column_index - 1]  # Get the \"Cell Tier\" cell\n",
    "        if cell.value in tier_colors:\n",
    "            fill = PatternFill(start_color=tier_colors[cell.value], end_color=tier_colors[cell.value], fill_type=\"solid\")\n",
    "            for cell_in_row in row:  # Apply color to the entire row\n",
    "                cell_in_row.fill = fill\n",
    "                cell_in_row.alignment = Alignment(horizontal='center', vertical='center')  # Center-align text\n",
    "\n",
    "# Auto-adjust column widths based on content length\n",
    "for col in ws.columns:\n",
    "    max_length = 0\n",
    "    col_letter = col[0].column_letter  # Get column letter (e.g., 'A', 'B', etc.)\n",
    "    for cell in col:\n",
    "        try:\n",
    "            if cell.value:\n",
    "                max_length = max(max_length, len(str(cell.value)))\n",
    "        except:\n",
    "            pass\n",
    "    adjusted_width = max_length + 2  # Add padding for readability\n",
    "    ws.column_dimensions[col_letter].width = adjusted_width\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(output_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
